<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><link rel="index" title="Index" href="../../../genindex.html" /><link rel="search" title="Search" href="../../../search.html" />

    <meta name="generator" content="sphinx-5.0.2, furo 2022.06.21"/>
        <title>pytorch_lightning.core.lightning - emonet</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo.css?digest=40978830699223671f4072448e654b5958f38b89" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../../index.html"><div class="brand">emonet</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../../index.html">
  
  
  <span class="sidebar-brand-text">emonet</span>
  
</a><form class="sidebar-search-container" method="get" action="../../../search.html" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quickstart.html#emonet">emonet</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../modules.html">emonet</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../emonet.html">emonet package</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/chris-santiago/emonet">Github Repo</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <h1>Source code for pytorch_lightning.core.lightning</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright The PyTorch Lightning team.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;The LightningModule - an nn.Module with many additional features.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">weakref</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">overload</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">ScriptModule</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">torch.optim.optimizer</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">torchmetrics</span> <span class="kn">import</span> <span class="n">Metric</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">Literal</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks.base</span> <span class="kn">import</span> <span class="n">Callback</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks.progress</span> <span class="kn">import</span> <span class="n">base</span> <span class="k">as</span> <span class="n">progress_base</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.core.hooks</span> <span class="kn">import</span> <span class="n">CheckpointHooks</span><span class="p">,</span> <span class="n">DataHooks</span><span class="p">,</span> <span class="n">ModelHooks</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.core.mixins</span> <span class="kn">import</span> <span class="n">DeviceDtypeModuleMixin</span><span class="p">,</span> <span class="n">HyperparametersMixin</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.core.optimizer</span> <span class="kn">import</span> <span class="n">LightningOptimizer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.core.saving</span> <span class="kn">import</span> <span class="n">ModelIO</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.loggers</span> <span class="kn">import</span> <span class="n">LightningLoggerBase</span><span class="p">,</span> <span class="n">LoggerCollection</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.trainer.connectors.data_connector</span> <span class="kn">import</span> <span class="n">_DataHookSelector</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.trainer.connectors.logger_connector.fx_validator</span> <span class="kn">import</span> <span class="n">_FxValidator</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities</span> <span class="kn">import</span> <span class="n">_IS_WINDOWS</span><span class="p">,</span> <span class="n">_TORCH_GREATER_EQUAL_1_10</span><span class="p">,</span> <span class="n">GradClipAlgorithmType</span><span class="p">,</span> <span class="n">warnings</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.apply_func</span> <span class="kn">import</span> <span class="n">apply_to_collection</span><span class="p">,</span> <span class="n">convert_to_tensors</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.cloud_io</span> <span class="kn">import</span> <span class="n">get_filesystem</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.distributed</span> <span class="kn">import</span> <span class="n">distributed_available</span><span class="p">,</span> <span class="n">sync_ddp</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.exceptions</span> <span class="kn">import</span> <span class="n">MisconfigurationException</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.imports</span> <span class="kn">import</span> <span class="n">_TORCH_GREATER_EQUAL_1_12</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.memory</span> <span class="kn">import</span> <span class="n">get_model_size_mb</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.model_summary</span> <span class="kn">import</span> <span class="n">ModelSummary</span><span class="p">,</span> <span class="n">summarize</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.parsing</span> <span class="kn">import</span> <span class="n">collect_init_args</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.rank_zero</span> <span class="kn">import</span> <span class="n">rank_zero_debug</span><span class="p">,</span> <span class="n">rank_zero_deprecation</span><span class="p">,</span> <span class="n">rank_zero_warn</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.signature_utils</span> <span class="kn">import</span> <span class="n">is_param_in_hook_signature</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.types</span> <span class="kn">import</span> <span class="n">_METRIC_COLLECTION</span><span class="p">,</span> <span class="n">EPOCH_OUTPUT</span><span class="p">,</span> <span class="n">LRSchedulerTypeUnion</span><span class="p">,</span> <span class="n">STEP_OUTPUT</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.warnings</span> <span class="kn">import</span> <span class="n">WarningCache</span>

<span class="n">warning_cache</span> <span class="o">=</span> <span class="n">WarningCache</span><span class="p">()</span>
<span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LightningModule</span><span class="p">(</span>
    <span class="n">DeviceDtypeModuleMixin</span><span class="p">,</span>
    <span class="n">HyperparametersMixin</span><span class="p">,</span>
    <span class="n">ModelIO</span><span class="p">,</span>
    <span class="n">ModelHooks</span><span class="p">,</span>
    <span class="n">DataHooks</span><span class="p">,</span>
    <span class="n">CheckpointHooks</span><span class="p">,</span>
    <span class="n">Module</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># Below is for property support of JIT</span>
    <span class="c1"># since none of these are important when using JIT, we are going to ignore them.</span>
    <span class="n">__jit_unused_properties__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">[</span>
            <span class="s2">&quot;example_input_array&quot;</span><span class="p">,</span>
            <span class="s2">&quot;on_gpu&quot;</span><span class="p">,</span>
            <span class="s2">&quot;current_epoch&quot;</span><span class="p">,</span>
            <span class="s2">&quot;global_step&quot;</span><span class="p">,</span>
            <span class="s2">&quot;global_rank&quot;</span><span class="p">,</span>
            <span class="s2">&quot;local_rank&quot;</span><span class="p">,</span>
            <span class="s2">&quot;logger&quot;</span><span class="p">,</span>
            <span class="s2">&quot;loggers&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model_size&quot;</span><span class="p">,</span>
            <span class="s2">&quot;automatic_optimization&quot;</span><span class="p">,</span>
            <span class="s2">&quot;truncated_bptt_steps&quot;</span><span class="p">,</span>
            <span class="s2">&quot;use_amp&quot;</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="o">+</span> <span class="n">DeviceDtypeModuleMixin</span><span class="o">.</span><span class="n">__jit_unused_properties__</span>
        <span class="o">+</span> <span class="n">HyperparametersMixin</span><span class="o">.</span><span class="n">__jit_unused_properties__</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># see (https://github.com/pytorch/pytorch/blob/3e6bb5233f9ca2c5aa55d9cda22a7ee85439aa6e/</span>
        <span class="c1"># torch/nn/modules/module.py#L227)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;lightning.module.</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># pointer to the trainer object</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;pl.Trainer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_use_amp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># the precision used</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>

        <span class="c1"># optionally can be set by user</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_example_input_array</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_optimization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_truncated_bptt_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_should_prevent_trainer_and_dataloaders_deepcopy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># TODO: remove in 1.8</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_register_sharded_tensor_state_dict_hooks_if_available</span><span class="p">()</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_pl_optimizer</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">LightningOptimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">LightningOptimizer</span><span class="p">]]:</span>
        <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_pl_optimizer</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">]]:</span>
        <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">optimizers</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">use_pl_optimizer</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">LightningOptimizer</span><span class="p">]]:</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">optimizers</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">use_pl_optimizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">LightningOptimizer</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the optimizer(s) that are being used during training. Useful for manual optimization.</span>

<span class="sd">        Args:</span>
<span class="sd">            use_pl_optimizer: If ``True``, will wrap the optimizer(s) in a</span>
<span class="sd">                :class:`~pytorch_lightning.core.optimizer.LightningOptimizer` for automatic handling of precision and</span>
<span class="sd">                profiling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A single optimizer, or a list of optimizers in case multiple ones are present.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">use_pl_optimizer</span><span class="p">:</span>
            <span class="n">opts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">_lightning_optimizers</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">opts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">optimizers</span>

        <span class="c1"># single optimizer</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">opts</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">opts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># multiple opts</span>
        <span class="k">return</span> <span class="n">opts</span>

    <span class="k">def</span> <span class="nf">lr_schedulers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">LRSchedulerTypeUnion</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">LRSchedulerTypeUnion</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the learning rate scheduler(s) that are being used during training. Useful for manual</span>
<span class="sd">        optimization.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no</span>
<span class="sd">            schedulers were returned in :meth:`configure_optimizers`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">lr_scheduler_configs</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># ignore other keys &quot;interval&quot;, &quot;frequency&quot;, etc.</span>
        <span class="n">lr_schedulers</span> <span class="o">=</span> <span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">scheduler</span> <span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">lr_scheduler_configs</span><span class="p">]</span>

        <span class="c1"># single scheduler</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_schedulers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">lr_schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># multiple schedulers</span>
        <span class="k">return</span> <span class="n">lr_schedulers</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">example_input_array</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The example input array is a specification of what the module can consume in the :meth:`forward` method.</span>
<span class="sd">        The return type is interpreted as follows:</span>

<span class="sd">        -   Single tensor: It is assumed the model takes a single argument, i.e.,</span>
<span class="sd">            ``model.forward(model.example_input_array)``</span>
<span class="sd">        -   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,</span>
<span class="sd">            ``model.forward(*model.example_input_array)``</span>
<span class="sd">        -   Dict: The input array represents named keyword arguments, i.e.,</span>
<span class="sd">            ``model.forward(**model.example_input_array)``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_example_input_array</span>

    <span class="nd">@example_input_array</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">example_input_array</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">example</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_example_input_array</span> <span class="o">=</span> <span class="n">example</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">current_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The current epoch in the ``Trainer``, or 0 if not attached.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">current_epoch</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">global_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Total training batches seen across all epochs.</span>

<span class="sd">        If no Trainer is attached, this propery is 0.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">global_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The index of the current process across all nodes and devices.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_rank</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">local_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The index of the current process within a single node.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">local_rank</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">on_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns ``True`` if this model is currently located on a GPU.</span>

<span class="sd">        Useful to set flags around the LightningModule for different CPU vs GPU behavior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">automatic_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_optimization</span>

    <span class="nd">@automatic_optimization</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">automatic_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">automatic_optimization</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_optimization</span> <span class="o">=</span> <span class="n">automatic_optimization</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">truncated_bptt_steps</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Enables `Truncated Backpropagation Through Time` in the Trainer when set to a positive integer.</span>

<span class="sd">        It represents</span>
<span class="sd">        the number of times :meth:`training_step` gets called before backpropagation. If this is &gt; 0, the</span>
<span class="sd">        :meth:`training_step` receives an additional argument ``hiddens`` and is expected to return a hidden state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_truncated_bptt_steps</span>

    <span class="nd">@truncated_bptt_steps</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">truncated_bptt_steps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">truncated_bptt_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_truncated_bptt_steps</span> <span class="o">=</span> <span class="n">truncated_bptt_steps</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">logger</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LightningLoggerBase</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Reference to the logger object in the Trainer.&quot;&quot;&quot;</span>
        <span class="c1"># this should match the implementation of `trainer.logger`</span>
        <span class="c1"># we don&#39;t reuse it so we can properly set the deprecation stacklevel</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">loggers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">loggers</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loggers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loggers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loggers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span><span class="p">:</span>
                <span class="n">rank_zero_deprecation</span><span class="p">(</span>
                    <span class="s2">&quot;Using `lightning_module.logger` when multiple loggers are configured.&quot;</span>
                    <span class="s2">&quot; This behavior will change in v1.8 when `LoggerCollection` is removed, and&quot;</span>
                    <span class="s2">&quot; `lightning_module.logger` will return the first logger available.&quot;</span><span class="p">,</span>
                    <span class="n">stacklevel</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">LoggerCollection</span><span class="p">(</span><span class="n">loggers</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">loggers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">LightningLoggerBase</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Reference to the list of loggers in the Trainer.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">loggers</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="k">else</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">_apply_batch_transfer_handler</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">device</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="n">datahook_selector</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">_DataHookSelector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">_data_connector</span><span class="o">.</span><span class="n">_datahook_selector</span>
        <span class="p">)</span>

        <span class="n">hook</span> <span class="o">=</span> <span class="n">datahook_selector</span><span class="o">.</span><span class="n">get_hook</span><span class="p">(</span><span class="s2">&quot;on_before_batch_transfer&quot;</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
        <span class="n">hook</span> <span class="o">=</span> <span class="n">datahook_selector</span><span class="o">.</span><span class="n">get_hook</span><span class="p">(</span><span class="s2">&quot;transfer_batch_to_device&quot;</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
        <span class="n">hook</span> <span class="o">=</span> <span class="n">datahook_selector</span><span class="o">.</span><span class="n">get_hook</span><span class="p">(</span><span class="s2">&quot;on_after_batch_transfer&quot;</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">print</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prints only from process 0. Use this in any distributed mode to log only once.</span>

<span class="sd">        Args:</span>
<span class="sd">            *args: The thing to print. The same as for Python&#39;s built-in print function.</span>
<span class="sd">            **kwargs: The same as for Python&#39;s built-in print function.</span>

<span class="sd">        Example::</span>

<span class="sd">            def forward(self, x):</span>
<span class="sd">                self.print(x, &#39;in forward&#39;)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">is_global_zero</span><span class="p">:</span>
            <span class="n">progress_bar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">progress_bar_callback</span>
            <span class="k">if</span> <span class="n">progress_bar</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">progress_bar</span><span class="o">.</span><span class="n">is_enabled</span><span class="p">:</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">log</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">_METRIC_COLLECTION</span><span class="p">,</span>
        <span class="n">prog_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">logger</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">on_step</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">on_epoch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce_fx</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
        <span class="n">enable_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sync_dist</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sync_dist_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_dataloader_idx</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metric_attribute</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rank_zero_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Log a key, value pair.</span>

<span class="sd">        Example::</span>

<span class="sd">            self.log(&#39;train_loss&#39;, loss)</span>

<span class="sd">        The default behavior per hook is documented here: :ref:`extensions/logging:Automatic Logging`.</span>

<span class="sd">        Args:</span>
<span class="sd">            name: key to log.</span>
<span class="sd">            value: value to log. Can be a ``float``, ``Tensor``, ``Metric``, or a dictionary of the former.</span>
<span class="sd">            prog_bar: if ``True`` logs to the progress bar.</span>
<span class="sd">            logger: if ``True`` logs to the logger.</span>
<span class="sd">            on_step: if ``True`` logs at this step. The default value is determined by the hook.</span>
<span class="sd">                See :ref:`extensions/logging:Automatic Logging` for details.</span>
<span class="sd">            on_epoch: if ``True`` logs epoch accumulated metrics. The default value is determined by the hook.</span>
<span class="sd">                See :ref:`extensions/logging:Automatic Logging` for details.</span>
<span class="sd">            reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.</span>
<span class="sd">            enable_graph: if ``True``, will not auto detach the graph.</span>
<span class="sd">            sync_dist: if ``True``, reduces the metric across devices. Use with care as this may lead to a significant</span>
<span class="sd">                communication overhead.</span>
<span class="sd">            sync_dist_group: the DDP group to sync across.</span>
<span class="sd">            add_dataloader_idx: if ``True``, appends the index of the current dataloader to</span>
<span class="sd">                the name (when using multiple dataloaders). If False, user needs to give unique names for</span>
<span class="sd">                each dataloader to not mix the values.</span>
<span class="sd">            batch_size: Current batch_size. This will be directly inferred from the loaded batch,</span>
<span class="sd">                but for some data structures you might need to explicitly provide it.</span>
<span class="sd">            metric_attribute: To restore the metric state, Lightning requires the reference of the</span>
<span class="sd">                :class:`torchmetrics.Metric` in your model. This is found automatically if it is a model attribute.</span>
<span class="sd">            rank_zero_only: Whether the value will be logged only on rank 0. This will prevent synchronization which</span>
<span class="sd">                would produce a deadlock as not all processes would perform this log call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># check for invalid values</span>
        <span class="n">apply_to_collection</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__check_not_nested</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="n">apply_to_collection</span><span class="p">(</span>
            <span class="n">value</span><span class="p">,</span> <span class="nb">object</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__check_allowed</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">wrong_dtype</span><span class="o">=</span><span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">,</span> <span class="n">Metric</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># not an error to support testing the `*_step` methods without a `Trainer` reference</span>
            <span class="n">rank_zero_warn</span><span class="p">(</span>
                <span class="s2">&quot;You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet.&quot;</span>
                <span class="s2">&quot; This is most likely because the model hasn&#39;t been passed to the `Trainer`&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span>
        <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">_results</span>
        <span class="k">if</span> <span class="n">results</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="s2">&quot;You are trying to `self.log()` but the loop&#39;s result collection is not registered&quot;</span>
                <span class="s2">&quot; yet. This is most likely because you are trying to log in a `predict` hook,&quot;</span>
                <span class="s2">&quot; but it doesn&#39;t support logging&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="s2">&quot;You are trying to `self.log()` but it is not managed by the `Trainer` control flow&quot;</span>
            <span class="p">)</span>

        <span class="n">on_step</span><span class="p">,</span> <span class="n">on_epoch</span> <span class="o">=</span> <span class="n">_FxValidator</span><span class="o">.</span><span class="n">check_logging_and_get_default_levels</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="n">on_step</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="n">on_epoch</span>
        <span class="p">)</span>

        <span class="c1"># make sure user doesn&#39;t introduce logic for multi-dataloaders</span>
        <span class="k">if</span> <span class="s2">&quot;/dataloader_idx_&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You called `self.log` with the key `</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">`&quot;</span>
                <span class="s2">&quot; but it should not contain information about `dataloader_idx`&quot;</span>
            <span class="p">)</span>

        <span class="n">value</span> <span class="o">=</span> <span class="n">apply_to_collection</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__to_tensor</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">_logger_connector</span><span class="o">.</span><span class="n">should_reset_tensors</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">):</span>
            <span class="c1"># if we started a new epoch (running its first batch) the hook name has changed</span>
            <span class="c1"># reset any tensors for the new hook name</span>
            <span class="n">results</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">metrics</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">metric_attribute</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Metric</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># compute once</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="nb">id</span><span class="p">(</span><span class="n">module</span><span class="p">):</span> <span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Metric</span><span class="p">)</span>
                <span class="p">}</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                        <span class="s2">&quot;Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.&quot;</span>
                        <span class="s2">&quot; You can fix this by setting an attribute for the metric in your `LightningModule`.&quot;</span>
                    <span class="p">)</span>
            <span class="c1"># try to find the passed metric in the LightningModule</span>
            <span class="n">metric_attribute</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">value</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">metric_attribute</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                    <span class="s2">&quot;Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; You can fix this by calling `self.log(</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">, ..., metric_attribute=name)` where `name` is one&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; of </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span><span class="o">.</span><span class="n">values</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">training</span>
            <span class="ow">and</span> <span class="n">is_param_in_hook_signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">,</span> <span class="s2">&quot;dataloader_iter&quot;</span><span class="p">,</span> <span class="n">explicit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="s2">&quot;With `def training_step(self, dataloader_iter)`, `self.log(..., batch_size=...)` should be provided.&quot;</span>
            <span class="p">)</span>

        <span class="n">results</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">,</span>
            <span class="n">name</span><span class="p">,</span>
            <span class="n">value</span><span class="p">,</span>
            <span class="n">prog_bar</span><span class="o">=</span><span class="n">prog_bar</span><span class="p">,</span>
            <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span>
            <span class="n">on_step</span><span class="o">=</span><span class="n">on_step</span><span class="p">,</span>
            <span class="n">on_epoch</span><span class="o">=</span><span class="n">on_epoch</span><span class="p">,</span>
            <span class="n">reduce_fx</span><span class="o">=</span><span class="n">reduce_fx</span><span class="p">,</span>
            <span class="n">enable_graph</span><span class="o">=</span><span class="n">enable_graph</span><span class="p">,</span>
            <span class="n">add_dataloader_idx</span><span class="o">=</span><span class="n">add_dataloader_idx</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">sync_dist</span><span class="o">=</span><span class="n">sync_dist</span> <span class="ow">and</span> <span class="n">distributed_available</span><span class="p">(),</span>
            <span class="n">sync_dist_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">reduce</span> <span class="ow">or</span> <span class="n">sync_ddp</span><span class="p">,</span>
            <span class="n">sync_dist_group</span><span class="o">=</span><span class="n">sync_dist_group</span><span class="p">,</span>
            <span class="n">metric_attribute</span><span class="o">=</span><span class="n">metric_attribute</span><span class="p">,</span>
            <span class="n">rank_zero_only</span><span class="o">=</span><span class="n">rank_zero_only</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">_logger_connector</span><span class="o">.</span><span class="n">_current_fx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span>

    <span class="k">def</span> <span class="nf">log_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dictionary</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_METRIC_COLLECTION</span><span class="p">],</span>
        <span class="n">prog_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">logger</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">on_step</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">on_epoch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce_fx</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
        <span class="n">enable_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sync_dist</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sync_dist_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_dataloader_idx</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rank_zero_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Log a dictionary of values at once.</span>

<span class="sd">        Example::</span>

<span class="sd">            values = {&#39;loss&#39;: loss, &#39;acc&#39;: acc, ..., &#39;metric_n&#39;: metric_n}</span>
<span class="sd">            self.log_dict(values)</span>

<span class="sd">        Args:</span>
<span class="sd">            dictionary: key value pairs.</span>
<span class="sd">                The values can be a ``float``, ``Tensor``, ``Metric``, or a dictionary of the former.</span>
<span class="sd">            prog_bar: if ``True`` logs to the progress base.</span>
<span class="sd">            logger: if ``True`` logs to the logger.</span>
<span class="sd">            on_step: if ``True`` logs at this step.</span>
<span class="sd">                ``None`` auto-logs for training_step but not validation/test_step.</span>
<span class="sd">                The default value is determined by the hook.</span>
<span class="sd">                See :ref:`extensions/logging:Automatic Logging` for details.</span>
<span class="sd">            on_epoch: if ``True`` logs epoch accumulated metrics.</span>
<span class="sd">                ``None`` auto-logs for val/test step but not ``training_step``.</span>
<span class="sd">                The default value is determined by the hook.</span>
<span class="sd">                See :ref:`extensions/logging:Automatic Logging` for details.</span>
<span class="sd">            reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.</span>
<span class="sd">            enable_graph: if ``True``, will not auto-detach the graph</span>
<span class="sd">            sync_dist: if ``True``, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant</span>
<span class="sd">                communication overhead.</span>
<span class="sd">            sync_dist_group: the ddp group to sync across.</span>
<span class="sd">            add_dataloader_idx: if ``True``, appends the index of the current dataloader to</span>
<span class="sd">                the name (when using multiple). If ``False``, user needs to give unique names for</span>
<span class="sd">                each dataloader to not mix values.</span>
<span class="sd">            batch_size: Current batch size. This will be directly inferred from the loaded batch,</span>
<span class="sd">                but some data structures might need to explicitly provide it.</span>
<span class="sd">            rank_zero_only: Whether the value will be logged only on rank 0. This will prevent synchronization which</span>
<span class="sd">                would produce a deadlock as not all processes would perform this log call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="n">k</span><span class="p">,</span>
                <span class="n">value</span><span class="o">=</span><span class="n">v</span><span class="p">,</span>
                <span class="n">prog_bar</span><span class="o">=</span><span class="n">prog_bar</span><span class="p">,</span>
                <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span>
                <span class="n">on_step</span><span class="o">=</span><span class="n">on_step</span><span class="p">,</span>
                <span class="n">on_epoch</span><span class="o">=</span><span class="n">on_epoch</span><span class="p">,</span>
                <span class="n">reduce_fx</span><span class="o">=</span><span class="n">reduce_fx</span><span class="p">,</span>
                <span class="n">enable_graph</span><span class="o">=</span><span class="n">enable_graph</span><span class="p">,</span>
                <span class="n">sync_dist</span><span class="o">=</span><span class="n">sync_dist</span><span class="p">,</span>
                <span class="n">sync_dist_group</span><span class="o">=</span><span class="n">sync_dist_group</span><span class="p">,</span>
                <span class="n">add_dataloader_idx</span><span class="o">=</span><span class="n">add_dataloader_idx</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">rank_zero_only</span><span class="o">=</span><span class="n">rank_zero_only</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">__check_not_nested</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># self-imposed restriction. for simplicity</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">value</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`self.log(</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">)` was called, but nested dictionaries cannot be logged&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">__check_allowed</span><span class="p">(</span><span class="n">v</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`self.log(</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">)` was called, but `</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">` values cannot be logged&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">log_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_norm_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Override this method to change the default behaviour of ``log_grad_norm``.</span>

<span class="sd">        If clipping gradients, the gradients will not have been clipped yet.</span>

<span class="sd">        Args:</span>
<span class="sd">            grad_norm_dict: Dictionary containing current grad norm metrics</span>

<span class="sd">        Example::</span>

<span class="sd">            # DEFAULT</span>
<span class="sd">            def log_grad_norm(self, grad_norm_dict):</span>
<span class="sd">                self.log_dict(grad_norm_dict, on_step=True, on_epoch=True, prog_bar=False, logger=True)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">grad_norm_dict</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">all_gather</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">],</span> <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sync_grads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Allows users to call ``self.all_gather()`` from the LightningModule, thus making the ``all_gather`` operation</span>
<span class="sd">        accelerator agnostic. ``all_gather`` is a function provided by accelerators to gather a tensor from several</span>
<span class="sd">        distributed processes.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.</span>
<span class="sd">            group: the process group to gather results from. Defaults to all processes (world)</span>
<span class="sd">            sync_grads: flag that allows users to synchronize gradients for the all_gather operation</span>

<span class="sd">        Return:</span>
<span class="sd">            A tensor of shape (world_size, batch, ...), or if the input was a collection</span>
<span class="sd">            the output will also be a collection with tensors of this shape.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">group</span> <span class="o">=</span> <span class="n">group</span> <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
        <span class="n">all_gather</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">all_gather</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">convert_to_tensors</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">apply_to_collection</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">all_gather</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span> <span class="n">sync_grads</span><span class="o">=</span><span class="n">sync_grads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Same as :meth:`torch.nn.Module.forward()`.</span>

<span class="sd">        Args:</span>
<span class="sd">            *args: Whatever you decide to pass into the forward method.</span>
<span class="sd">            **kwargs: Keyword arguments are also possible.</span>

<span class="sd">        Return:</span>
<span class="sd">            Your model&#39;s output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">STEP_OUTPUT</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Here you compute and return the training loss and some additional metrics for e.g.</span>
<span class="sd">        the progress bar or logger.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch (:class:`~torch.Tensor` | (:class:`~torch.Tensor`, ...) | [:class:`~torch.Tensor`, ...]):</span>
<span class="sd">                The output of your :class:`~torch.utils.data.DataLoader`. A tensor, tuple or list.</span>
<span class="sd">            batch_idx (``int``): Integer displaying index of this batch</span>
<span class="sd">            optimizer_idx (``int``): When using multiple optimizers, this argument will also be present.</span>
<span class="sd">            hiddens (``Any``): Passed in if</span>
<span class="sd">                :paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps` &gt; 0.</span>

<span class="sd">        Return:</span>
<span class="sd">            Any of.</span>

<span class="sd">            - :class:`~torch.Tensor` - The loss tensor</span>
<span class="sd">            - ``dict`` - A dictionary. Can include any keys, but must include the key ``&#39;loss&#39;``</span>
<span class="sd">            - ``None`` - Training will skip to the next batch. This is only for automatic optimization.</span>
<span class="sd">                This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.</span>

<span class="sd">        In this step you&#39;d normally do the forward pass and calculate the loss for a batch.</span>
<span class="sd">        You can also do fancier things like multiple forward passes or something model specific.</span>

<span class="sd">        Example::</span>

<span class="sd">            def training_step(self, batch, batch_idx):</span>
<span class="sd">                x, y, z = batch</span>
<span class="sd">                out = self.encoder(x)</span>
<span class="sd">                loss = self.loss(out, x)</span>
<span class="sd">                return loss</span>

<span class="sd">        If you define multiple optimizers, this step will be called with an additional</span>
<span class="sd">        ``optimizer_idx`` parameter.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # Multiple optimizers (e.g.: GANs)</span>
<span class="sd">            def training_step(self, batch, batch_idx, optimizer_idx):</span>
<span class="sd">                if optimizer_idx == 0:</span>
<span class="sd">                    # do training_step with encoder</span>
<span class="sd">                    ...</span>
<span class="sd">                if optimizer_idx == 1:</span>
<span class="sd">                    # do training_step with decoder</span>
<span class="sd">                    ...</span>


<span class="sd">        If you add truncated back propagation through time you will also get an additional</span>
<span class="sd">        argument with the hidden states of the previous step.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # Truncated back-propagation through time</span>
<span class="sd">            def training_step(self, batch, batch_idx, hiddens):</span>
<span class="sd">                # hiddens are the hidden states from the previous truncated backprop step</span>
<span class="sd">                out, hiddens = self.lstm(data, hiddens)</span>
<span class="sd">                loss = ...</span>
<span class="sd">                return {&quot;loss&quot;: loss, &quot;hiddens&quot;: hiddens}</span>

<span class="sd">        Note:</span>
<span class="sd">            The loss value shown in the progress bar is smoothed (averaged) over the last values,</span>
<span class="sd">            so it differs from the actual loss returned in train/validation step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">rank_zero_warn</span><span class="p">(</span><span class="s2">&quot;`training_step` must be implemented to be used with the Lightning Trainer&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step_output</span><span class="p">:</span> <span class="n">STEP_OUTPUT</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">STEP_OUTPUT</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Use this when training with dp or ddp2 because :meth:`training_step` will operate on only part of the</span>
<span class="sd">        batch. However, this is still optional and only needed for things like softmax or NCE loss.</span>

<span class="sd">        Note:</span>
<span class="sd">            If you later switch to ddp or some other mode, this will still be called</span>
<span class="sd">            so that you don&#39;t have to change your code</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # pseudocode</span>
<span class="sd">            sub_batches = split_batches_for_dp(batch)</span>
<span class="sd">            step_output = [training_step(sub_batch) for sub_batch in sub_batches]</span>
<span class="sd">            training_step_end(step_output)</span>

<span class="sd">        Args:</span>
<span class="sd">            step_output: What you return in `training_step` for each batch part.</span>

<span class="sd">        Return:</span>
<span class="sd">            Anything</span>

<span class="sd">        When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            def training_step(self, batch, batch_idx):</span>
<span class="sd">                # batch is 1/num_gpus big</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                out = self(x)</span>

<span class="sd">                # softmax uses only a portion of the batch in the denominator</span>
<span class="sd">                loss = self.softmax(out)</span>
<span class="sd">                loss = nce_loss(loss)</span>
<span class="sd">                return loss</span>

<span class="sd">        If you wish to do something with all the parts of the batch, then use this method to do it:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            def training_step(self, batch, batch_idx):</span>
<span class="sd">                # batch is 1/num_gpus big</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                out = self.encoder(x)</span>
<span class="sd">                return {&quot;pred&quot;: out}</span>


<span class="sd">            def training_step_end(self, training_step_outputs):</span>
<span class="sd">                gpu_0_pred = training_step_outputs[0][&quot;pred&quot;]</span>
<span class="sd">                gpu_1_pred = training_step_outputs[1][&quot;pred&quot;]</span>
<span class="sd">                gpu_n_pred = training_step_outputs[n][&quot;pred&quot;]</span>

<span class="sd">                # this softmax now uses the full batch</span>
<span class="sd">                loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred])</span>
<span class="sd">                return loss</span>

<span class="sd">        See Also:</span>
<span class="sd">            See the :ref:`accelerators/gpu:Multi GPU Training` guide for more details.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">EPOCH_OUTPUT</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Called at the end of the training epoch with the outputs of all training steps. Use this in case you</span>
<span class="sd">        need to do something with all the outputs returned by :meth:`training_step`.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # the pseudocode for these calls</span>
<span class="sd">            train_outs = []</span>
<span class="sd">            for train_batch in train_data:</span>
<span class="sd">                out = training_step(train_batch)</span>
<span class="sd">                train_outs.append(out)</span>
<span class="sd">            training_epoch_end(train_outs)</span>

<span class="sd">        Args:</span>
<span class="sd">            outputs: List of outputs you defined in :meth:`training_step`. If there are multiple optimizers or when</span>
<span class="sd">                using ``truncated_bptt_steps &gt; 0``, the lists have the dimensions</span>
<span class="sd">                (n_batches, tbptt_steps, n_optimizers). Dimensions of length 1 are squeezed.</span>

<span class="sd">        Return:</span>
<span class="sd">            None</span>

<span class="sd">        Note:</span>
<span class="sd">            If this method is not overridden, this won&#39;t be called.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            def training_epoch_end(self, training_step_outputs):</span>
<span class="sd">                # do something with all training_step outputs</span>
<span class="sd">                for out in training_step_outputs:</span>
<span class="sd">                    ...</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STEP_OUTPUT</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Operates on a single batch of data from the validation set.</span>
<span class="sd">        In this step you&#39;d might generate examples or calculate anything of interest like accuracy.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # the pseudocode for these calls</span>
<span class="sd">            val_outs = []</span>
<span class="sd">            for val_batch in val_data:</span>
<span class="sd">                out = validation_step(val_batch)</span>
<span class="sd">                val_outs.append(out)</span>
<span class="sd">            validation_epoch_end(val_outs)</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: The output of your :class:`~torch.utils.data.DataLoader`.</span>
<span class="sd">            batch_idx: The index of this batch.</span>
<span class="sd">            dataloader_idx: The index of the dataloader that produced this batch.</span>
<span class="sd">                (only if multiple val dataloaders used)</span>

<span class="sd">        Return:</span>
<span class="sd">            - Any object or value</span>
<span class="sd">            - ``None`` - Validation will skip to the next batch</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # pseudocode of order</span>
<span class="sd">            val_outs = []</span>
<span class="sd">            for val_batch in val_data:</span>
<span class="sd">                out = validation_step(val_batch)</span>
<span class="sd">                if defined(&quot;validation_step_end&quot;):</span>
<span class="sd">                    out = validation_step_end(out)</span>
<span class="sd">                val_outs.append(out)</span>
<span class="sd">            val_outs = validation_epoch_end(val_outs)</span>


<span class="sd">        .. code-block:: python</span>

<span class="sd">            # if you have one val dataloader:</span>
<span class="sd">            def validation_step(self, batch, batch_idx):</span>
<span class="sd">                ...</span>


<span class="sd">            # if you have multiple val dataloaders:</span>
<span class="sd">            def validation_step(self, batch, batch_idx, dataloader_idx=0):</span>
<span class="sd">                ...</span>

<span class="sd">        Examples::</span>

<span class="sd">            # CASE 1: A single validation dataset</span>
<span class="sd">            def validation_step(self, batch, batch_idx):</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                # implement your own</span>
<span class="sd">                out = self(x)</span>
<span class="sd">                loss = self.loss(out, y)</span>

<span class="sd">                # log 6 example images</span>
<span class="sd">                # or generated text... or whatever</span>
<span class="sd">                sample_imgs = x[:6]</span>
<span class="sd">                grid = torchvision.utils.make_grid(sample_imgs)</span>
<span class="sd">                self.logger.experiment.add_image(&#39;example_images&#39;, grid, 0)</span>

<span class="sd">                # calculate acc</span>
<span class="sd">                labels_hat = torch.argmax(out, dim=1)</span>
<span class="sd">                val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)</span>

<span class="sd">                # log the outputs!</span>
<span class="sd">                self.log_dict({&#39;val_loss&#39;: loss, &#39;val_acc&#39;: val_acc})</span>

<span class="sd">        If you pass in multiple val dataloaders, :meth:`validation_step` will have an additional argument. We recommend</span>
<span class="sd">        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # CASE 2: multiple validation dataloaders</span>
<span class="sd">            def validation_step(self, batch, batch_idx, dataloader_idx=0):</span>
<span class="sd">                # dataloader_idx tells you which dataset this is.</span>
<span class="sd">                ...</span>

<span class="sd">        Note:</span>
<span class="sd">            If you don&#39;t need to validate you don&#39;t need to implement this method.</span>

<span class="sd">        Note:</span>
<span class="sd">            When the :meth:`validation_step` is called, the model has been put in eval mode</span>
<span class="sd">            and PyTorch gradients have been disabled. At the end of validation,</span>
<span class="sd">            the model goes back to training mode and gradients are enabled.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">validation_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STEP_OUTPUT</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Use this when validating with dp or ddp2 because :meth:`validation_step` will operate on only part of</span>
<span class="sd">        the batch. However, this is still optional and only needed for things like softmax or NCE loss.</span>

<span class="sd">        Note:</span>
<span class="sd">            If you later switch to ddp or some other mode, this will still be called</span>
<span class="sd">            so that you don&#39;t have to change your code.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # pseudocode</span>
<span class="sd">            sub_batches = split_batches_for_dp(batch)</span>
<span class="sd">            step_output = [validation_step(sub_batch) for sub_batch in sub_batches]</span>
<span class="sd">            validation_step_end(step_output)</span>

<span class="sd">        Args:</span>
<span class="sd">            step_output: What you return in :meth:`validation_step` for each batch part.</span>

<span class="sd">        Return:</span>
<span class="sd">            None or anything</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # WITHOUT validation_step_end</span>
<span class="sd">            # if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="sd">            def validation_step(self, batch, batch_idx):</span>
<span class="sd">                # batch is 1/num_gpus big</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                out = self.encoder(x)</span>
<span class="sd">                loss = self.softmax(out)</span>
<span class="sd">                loss = nce_loss(loss)</span>
<span class="sd">                self.log(&quot;val_loss&quot;, loss)</span>


<span class="sd">            # --------------</span>
<span class="sd">            # with validation_step_end to do softmax over the full batch</span>
<span class="sd">            def validation_step(self, batch, batch_idx):</span>
<span class="sd">                # batch is 1/num_gpus big</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                out = self(x)</span>
<span class="sd">                return out</span>


<span class="sd">            def validation_step_end(self, val_step_outputs):</span>
<span class="sd">                for out in val_step_outputs:</span>
<span class="sd">                    ...</span>

<span class="sd">        See Also:</span>
<span class="sd">            See the :ref:`accelerators/gpu:Multi GPU Training` guide for more details.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">EPOCH_OUTPUT</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">EPOCH_OUTPUT</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Called at the end of the validation epoch with the outputs of all validation steps.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # the pseudocode for these calls</span>
<span class="sd">            val_outs = []</span>
<span class="sd">            for val_batch in val_data:</span>
<span class="sd">                out = validation_step(val_batch)</span>
<span class="sd">                val_outs.append(out)</span>
<span class="sd">            validation_epoch_end(val_outs)</span>

<span class="sd">        Args:</span>
<span class="sd">            outputs: List of outputs you defined in :meth:`validation_step`, or if there</span>
<span class="sd">                are multiple dataloaders, a list containing a list of outputs for each dataloader.</span>

<span class="sd">        Return:</span>
<span class="sd">            None</span>

<span class="sd">        Note:</span>
<span class="sd">            If you didn&#39;t define a :meth:`validation_step`, this won&#39;t be called.</span>

<span class="sd">        Examples:</span>
<span class="sd">            With a single dataloader:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def validation_epoch_end(self, val_step_outputs):</span>
<span class="sd">                    for out in val_step_outputs:</span>
<span class="sd">                        ...</span>

<span class="sd">            With multiple dataloaders, `outputs` will be a list of lists. The outer list contains</span>
<span class="sd">            one entry per dataloader, while the inner list contains the individual outputs of</span>
<span class="sd">            each validation step for that dataloader.</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def validation_epoch_end(self, outputs):</span>
<span class="sd">                    for dataloader_output_result in outputs:</span>
<span class="sd">                        dataloader_outs = dataloader_output_result.dataloader_i_outputs</span>

<span class="sd">                    self.log(&quot;final_metric&quot;, final_value)</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STEP_OUTPUT</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Operates on a single batch of data from the test set.</span>
<span class="sd">        In this step you&#39;d normally generate examples or calculate anything of interest</span>
<span class="sd">        such as accuracy.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # the pseudocode for these calls</span>
<span class="sd">            test_outs = []</span>
<span class="sd">            for test_batch in test_data:</span>
<span class="sd">                out = test_step(test_batch)</span>
<span class="sd">                test_outs.append(out)</span>
<span class="sd">            test_epoch_end(test_outs)</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: The output of your :class:`~torch.utils.data.DataLoader`.</span>
<span class="sd">            batch_idx: The index of this batch.</span>
<span class="sd">            dataloader_id: The index of the dataloader that produced this batch.</span>
<span class="sd">                (only if multiple test dataloaders used).</span>

<span class="sd">        Return:</span>
<span class="sd">           Any of.</span>

<span class="sd">            - Any object or value</span>
<span class="sd">            - ``None`` - Testing will skip to the next batch</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # if you have one test dataloader:</span>
<span class="sd">            def test_step(self, batch, batch_idx):</span>
<span class="sd">                ...</span>


<span class="sd">            # if you have multiple test dataloaders:</span>
<span class="sd">            def test_step(self, batch, batch_idx, dataloader_idx=0):</span>
<span class="sd">                ...</span>

<span class="sd">        Examples::</span>

<span class="sd">            # CASE 1: A single test dataset</span>
<span class="sd">            def test_step(self, batch, batch_idx):</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                # implement your own</span>
<span class="sd">                out = self(x)</span>
<span class="sd">                loss = self.loss(out, y)</span>

<span class="sd">                # log 6 example images</span>
<span class="sd">                # or generated text... or whatever</span>
<span class="sd">                sample_imgs = x[:6]</span>
<span class="sd">                grid = torchvision.utils.make_grid(sample_imgs)</span>
<span class="sd">                self.logger.experiment.add_image(&#39;example_images&#39;, grid, 0)</span>

<span class="sd">                # calculate acc</span>
<span class="sd">                labels_hat = torch.argmax(out, dim=1)</span>
<span class="sd">                test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)</span>

<span class="sd">                # log the outputs!</span>
<span class="sd">                self.log_dict({&#39;test_loss&#39;: loss, &#39;test_acc&#39;: test_acc})</span>

<span class="sd">        If you pass in multiple test dataloaders, :meth:`test_step` will have an additional argument. We recommend</span>
<span class="sd">        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # CASE 2: multiple test dataloaders</span>
<span class="sd">            def test_step(self, batch, batch_idx, dataloader_idx=0):</span>
<span class="sd">                # dataloader_idx tells you which dataset this is.</span>
<span class="sd">                ...</span>

<span class="sd">        Note:</span>
<span class="sd">            If you don&#39;t need to test you don&#39;t need to implement this method.</span>

<span class="sd">        Note:</span>
<span class="sd">            When the :meth:`test_step` is called, the model has been put in eval mode and</span>
<span class="sd">            PyTorch gradients have been disabled. At the end of the test epoch, the model goes back</span>
<span class="sd">            to training mode and gradients are enabled.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">test_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STEP_OUTPUT</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Use this when testing with dp or ddp2 because :meth:`test_step` will operate on only part of the batch.</span>
<span class="sd">        However, this is still optional and only needed for things like softmax or NCE loss.</span>

<span class="sd">        Note:</span>
<span class="sd">            If you later switch to ddp or some other mode, this will still be called</span>
<span class="sd">            so that you don&#39;t have to change your code.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # pseudocode</span>
<span class="sd">            sub_batches = split_batches_for_dp(batch)</span>
<span class="sd">            step_output = [test_step(sub_batch) for sub_batch in sub_batches]</span>
<span class="sd">            test_step_end(step_output)</span>

<span class="sd">        Args:</span>
<span class="sd">            step_output: What you return in :meth:`test_step` for each batch part.</span>

<span class="sd">        Return:</span>
<span class="sd">            None or anything</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # WITHOUT test_step_end</span>
<span class="sd">            # if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="sd">            def test_step(self, batch, batch_idx):</span>
<span class="sd">                # batch is 1/num_gpus big</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                out = self(x)</span>
<span class="sd">                loss = self.softmax(out)</span>
<span class="sd">                self.log(&quot;test_loss&quot;, loss)</span>


<span class="sd">            # --------------</span>
<span class="sd">            # with test_step_end to do softmax over the full batch</span>
<span class="sd">            def test_step(self, batch, batch_idx):</span>
<span class="sd">                # batch is 1/num_gpus big</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                out = self.encoder(x)</span>
<span class="sd">                return out</span>


<span class="sd">            def test_step_end(self, output_results):</span>
<span class="sd">                # this out is now the full size of the batch</span>
<span class="sd">                all_test_step_outs = output_results.out</span>
<span class="sd">                loss = nce_loss(all_test_step_outs)</span>
<span class="sd">                self.log(&quot;test_loss&quot;, loss)</span>

<span class="sd">        See Also:</span>
<span class="sd">            See the :ref:`accelerators/gpu:Multi GPU Training` guide for more details.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">EPOCH_OUTPUT</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">EPOCH_OUTPUT</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Called at the end of a test epoch with the output of all test steps.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # the pseudocode for these calls</span>
<span class="sd">            test_outs = []</span>
<span class="sd">            for test_batch in test_data:</span>
<span class="sd">                out = test_step(test_batch)</span>
<span class="sd">                test_outs.append(out)</span>
<span class="sd">            test_epoch_end(test_outs)</span>

<span class="sd">        Args:</span>
<span class="sd">            outputs: List of outputs you defined in :meth:`test_step_end`, or if there</span>
<span class="sd">                are multiple dataloaders, a list containing a list of outputs for each dataloader</span>

<span class="sd">        Return:</span>
<span class="sd">            None</span>

<span class="sd">        Note:</span>
<span class="sd">            If you didn&#39;t define a :meth:`test_step`, this won&#39;t be called.</span>

<span class="sd">        Examples:</span>
<span class="sd">            With a single dataloader:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def test_epoch_end(self, outputs):</span>
<span class="sd">                    # do something with the outputs of all test batches</span>
<span class="sd">                    all_test_preds = test_step_outputs.predictions</span>

<span class="sd">                    some_result = calc_all_results(all_test_preds)</span>
<span class="sd">                    self.log(some_result)</span>

<span class="sd">            With multiple dataloaders, `outputs` will be a list of lists. The outer list contains</span>
<span class="sd">            one entry per dataloader, while the inner list contains the individual outputs of</span>
<span class="sd">            each test step for that dataloader.</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def test_epoch_end(self, outputs):</span>
<span class="sd">                    final_value = 0</span>
<span class="sd">                    for dataloader_outputs in outputs:</span>
<span class="sd">                        for test_step_out in dataloader_outputs:</span>
<span class="sd">                            # do something</span>
<span class="sd">                            final_value += test_step_out</span>

<span class="sd">                    self.log(&quot;final_metric&quot;, final_value)</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Step function called during :meth:`~pytorch_lightning.trainer.trainer.Trainer.predict`. By default, it</span>
<span class="sd">        calls :meth:`~pytorch_lightning.core.lightning.LightningModule.forward`. Override to add any processing</span>
<span class="sd">        logic.</span>

<span class="sd">        The :meth:`~pytorch_lightning.core.lightning.LightningModule.predict_step` is used</span>
<span class="sd">        to scale inference on multi-devices.</span>

<span class="sd">        To prevent an OOM error, it is possible to use :class:`~pytorch_lightning.callbacks.BasePredictionWriter`</span>
<span class="sd">        callback to write the predictions to disk or database after each batch or on epoch end.</span>

<span class="sd">        The :class:`~pytorch_lightning.callbacks.BasePredictionWriter` should be used while using a spawn</span>
<span class="sd">        based accelerator. This happens for ``Trainer(strategy=&quot;ddp_spawn&quot;)``</span>
<span class="sd">        or training on 8 TPU cores with ``Trainer(accelerator=&quot;tpu&quot;, devices=8)`` as predictions won&#39;t be returned.</span>

<span class="sd">        Example ::</span>

<span class="sd">            class MyModel(LightningModule):</span>

<span class="sd">                def predicts_step(self, batch, batch_idx, dataloader_idx=0):</span>
<span class="sd">                    return self(batch)</span>

<span class="sd">            dm = ...</span>
<span class="sd">            model = MyModel()</span>
<span class="sd">            trainer = Trainer(accelerator=&quot;gpu&quot;, devices=2)</span>
<span class="sd">            predictions = trainer.predict(model, dm)</span>


<span class="sd">        Args:</span>
<span class="sd">            batch: Current batch.</span>
<span class="sd">            batch_idx: Index of current batch.</span>
<span class="sd">            dataloader_idx: Index of the current dataloader.</span>

<span class="sd">        Return:</span>
<span class="sd">            Predicted output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Callback</span><span class="p">],</span> <span class="n">Callback</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()``</span>
<span class="sd">        gets called, the list or a callback returned here will be merged with the list of callbacks passed to the</span>
<span class="sd">        Trainer&#39;s ``callbacks`` argument. If a callback returned here has the same type as one or several callbacks</span>
<span class="sd">        already present in the Trainer&#39;s callbacks list, it will take priority and replace them. In addition,</span>
<span class="sd">        Lightning will make sure :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` callbacks</span>
<span class="sd">        run last.</span>

<span class="sd">        Return:</span>
<span class="sd">            A callback or a list of callbacks which will extend the list of callbacks in the Trainer.</span>

<span class="sd">        Example::</span>

<span class="sd">            def configure_callbacks(self):</span>
<span class="sd">                early_stop = EarlyStopping(monitor=&quot;val_acc&quot;, mode=&quot;max&quot;)</span>
<span class="sd">                checkpoint = ModelCheckpoint(monitor=&quot;val_loss&quot;)</span>
<span class="sd">                return [early_stop, checkpoint]</span>

<span class="sd">        Note:</span>
<span class="sd">            Certain callback methods like :meth:`~pytorch_lightning.callbacks.base.Callback.on_init_start`</span>
<span class="sd">            will never be invoked on the new callbacks returned here.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Choose what optimizers and learning-rate schedulers to use in your optimization.</span>
<span class="sd">        Normally you&#39;d need one. But in the case of GANs or similar you might have multiple.</span>

<span class="sd">        Return:</span>
<span class="sd">            Any of these 6 options.</span>

<span class="sd">            - **Single optimizer**.</span>
<span class="sd">            - **List or Tuple** of optimizers.</span>
<span class="sd">            - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers</span>
<span class="sd">              (or multiple ``lr_scheduler_config``).</span>
<span class="sd">            - **Dictionary**, with an ``&quot;optimizer&quot;`` key, and (optionally) a ``&quot;lr_scheduler&quot;``</span>
<span class="sd">              key whose value is a single LR scheduler or ``lr_scheduler_config``.</span>
<span class="sd">            - **Tuple of dictionaries** as described above, with an optional ``&quot;frequency&quot;`` key.</span>
<span class="sd">            - **None** - Fit will run without any optimizer.</span>

<span class="sd">        The ``lr_scheduler_config`` is a dictionary which contains the scheduler and its associated configuration.</span>
<span class="sd">        The default configuration is shown below.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            lr_scheduler_config = {</span>
<span class="sd">                # REQUIRED: The scheduler instance</span>
<span class="sd">                &quot;scheduler&quot;: lr_scheduler,</span>
<span class="sd">                # The unit of the scheduler&#39;s step size, could also be &#39;step&#39;.</span>
<span class="sd">                # &#39;epoch&#39; updates the scheduler on epoch end whereas &#39;step&#39;</span>
<span class="sd">                # updates it after a optimizer update.</span>
<span class="sd">                &quot;interval&quot;: &quot;epoch&quot;,</span>
<span class="sd">                # How many epochs/steps should pass between calls to</span>
<span class="sd">                # `scheduler.step()`. 1 corresponds to updating the learning</span>
<span class="sd">                # rate after every epoch/step.</span>
<span class="sd">                &quot;frequency&quot;: 1,</span>
<span class="sd">                # Metric to to monitor for schedulers like `ReduceLROnPlateau`</span>
<span class="sd">                &quot;monitor&quot;: &quot;val_loss&quot;,</span>
<span class="sd">                # If set to `True`, will enforce that the value specified &#39;monitor&#39;</span>
<span class="sd">                # is available when the scheduler is updated, thus stopping</span>
<span class="sd">                # training if not found. If set to `False`, it will only produce a warning</span>
<span class="sd">                &quot;strict&quot;: True,</span>
<span class="sd">                # If using the `LearningRateMonitor` callback to monitor the</span>
<span class="sd">                # learning rate progress, this keyword can be used to specify</span>
<span class="sd">                # a custom logged name</span>
<span class="sd">                &quot;name&quot;: None,</span>
<span class="sd">            }</span>

<span class="sd">        When there are schedulers in which the ``.step()`` method is conditioned on a value, such as the</span>
<span class="sd">        :class:`torch.optim.lr_scheduler.ReduceLROnPlateau` scheduler, Lightning requires that the</span>
<span class="sd">        ``lr_scheduler_config`` contains the keyword ``&quot;monitor&quot;`` set to the metric name that the scheduler</span>
<span class="sd">        should be conditioned on.</span>

<span class="sd">        .. testcode::</span>

<span class="sd">            # The ReduceLROnPlateau scheduler requires a monitor</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                optimizer = Adam(...)</span>
<span class="sd">                return {</span>
<span class="sd">                    &quot;optimizer&quot;: optimizer,</span>
<span class="sd">                    &quot;lr_scheduler&quot;: {</span>
<span class="sd">                        &quot;scheduler&quot;: ReduceLROnPlateau(optimizer, ...),</span>
<span class="sd">                        &quot;monitor&quot;: &quot;metric_to_track&quot;,</span>
<span class="sd">                        &quot;frequency&quot;: &quot;indicates how often the metric is updated&quot;</span>
<span class="sd">                        # If &quot;monitor&quot; references validation metrics, then &quot;frequency&quot; should be set to a</span>
<span class="sd">                        # multiple of &quot;trainer.check_val_every_n_epoch&quot;.</span>
<span class="sd">                    },</span>
<span class="sd">                }</span>


<span class="sd">            # In the case of two optimizers, only one using the ReduceLROnPlateau scheduler</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                optimizer1 = Adam(...)</span>
<span class="sd">                optimizer2 = SGD(...)</span>
<span class="sd">                scheduler1 = ReduceLROnPlateau(optimizer1, ...)</span>
<span class="sd">                scheduler2 = LambdaLR(optimizer2, ...)</span>
<span class="sd">                return (</span>
<span class="sd">                    {</span>
<span class="sd">                        &quot;optimizer&quot;: optimizer1,</span>
<span class="sd">                        &quot;lr_scheduler&quot;: {</span>
<span class="sd">                            &quot;scheduler&quot;: scheduler1,</span>
<span class="sd">                            &quot;monitor&quot;: &quot;metric_to_track&quot;,</span>
<span class="sd">                        },</span>
<span class="sd">                    },</span>
<span class="sd">                    {&quot;optimizer&quot;: optimizer2, &quot;lr_scheduler&quot;: scheduler2},</span>
<span class="sd">                )</span>

<span class="sd">        Metrics can be made available to monitor by simply logging it using</span>
<span class="sd">        ``self.log(&#39;metric_to_track&#39;, metric_val)`` in your :class:`~pytorch_lightning.core.lightning.LightningModule`.</span>

<span class="sd">        Note:</span>
<span class="sd">            The ``frequency`` value specified in a dict along with the ``optimizer`` key is an int corresponding</span>
<span class="sd">            to the number of sequential batches optimized with the specific optimizer.</span>
<span class="sd">            It should be given to none or to all of the optimizers.</span>
<span class="sd">            There is a difference between passing multiple optimizers in a list,</span>
<span class="sd">            and passing multiple optimizers in dictionaries with a frequency of 1:</span>

<span class="sd">                - In the former case, all optimizers will operate on the given batch in each optimization step.</span>
<span class="sd">                - In the latter, only one optimizer will operate on the given batch at every step.</span>

<span class="sd">            This is different from the ``frequency`` value specified in the ``lr_scheduler_config`` mentioned above.</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def configure_optimizers(self):</span>
<span class="sd">                    optimizer_one = torch.optim.SGD(self.model.parameters(), lr=0.01)</span>
<span class="sd">                    optimizer_two = torch.optim.SGD(self.model.parameters(), lr=0.01)</span>
<span class="sd">                    return [</span>
<span class="sd">                        {&quot;optimizer&quot;: optimizer_one, &quot;frequency&quot;: 5},</span>
<span class="sd">                        {&quot;optimizer&quot;: optimizer_two, &quot;frequency&quot;: 10},</span>
<span class="sd">                    ]</span>

<span class="sd">            In this example, the first optimizer will be used for the first 5 steps,</span>
<span class="sd">            the second optimizer for the next 10 steps and that cycle will continue.</span>
<span class="sd">            If an LR scheduler is specified for an optimizer using the ``lr_scheduler`` key in the above dict,</span>
<span class="sd">            the scheduler will only be updated when its optimizer is being used.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # most cases. no learning rate scheduler</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                return Adam(self.parameters(), lr=1e-3)</span>

<span class="sd">            # multiple optimizer case (e.g.: GAN)</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                gen_opt = Adam(self.model_gen.parameters(), lr=0.01)</span>
<span class="sd">                dis_opt = Adam(self.model_dis.parameters(), lr=0.02)</span>
<span class="sd">                return gen_opt, dis_opt</span>

<span class="sd">            # example with learning rate schedulers</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                gen_opt = Adam(self.model_gen.parameters(), lr=0.01)</span>
<span class="sd">                dis_opt = Adam(self.model_dis.parameters(), lr=0.02)</span>
<span class="sd">                dis_sch = CosineAnnealing(dis_opt, T_max=10)</span>
<span class="sd">                return [gen_opt, dis_opt], [dis_sch]</span>

<span class="sd">            # example with step-based learning rate schedulers</span>
<span class="sd">            # each optimizer has its own scheduler</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                gen_opt = Adam(self.model_gen.parameters(), lr=0.01)</span>
<span class="sd">                dis_opt = Adam(self.model_dis.parameters(), lr=0.02)</span>
<span class="sd">                gen_sch = {</span>
<span class="sd">                    &#39;scheduler&#39;: ExponentialLR(gen_opt, 0.99),</span>
<span class="sd">                    &#39;interval&#39;: &#39;step&#39;  # called after each training step</span>
<span class="sd">                }</span>
<span class="sd">                dis_sch = CosineAnnealing(dis_opt, T_max=10) # called every epoch</span>
<span class="sd">                return [gen_opt, dis_opt], [gen_sch, dis_sch]</span>

<span class="sd">            # example with optimizer frequencies</span>
<span class="sd">            # see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="sd">            # https://arxiv.org/abs/1704.00028</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                gen_opt = Adam(self.model_gen.parameters(), lr=0.01)</span>
<span class="sd">                dis_opt = Adam(self.model_dis.parameters(), lr=0.02)</span>
<span class="sd">                n_critic = 5</span>
<span class="sd">                return (</span>
<span class="sd">                    {&#39;optimizer&#39;: dis_opt, &#39;frequency&#39;: n_critic},</span>
<span class="sd">                    {&#39;optimizer&#39;: gen_opt, &#39;frequency&#39;: 1}</span>
<span class="sd">                )</span>

<span class="sd">        Note:</span>
<span class="sd">            Some things to know:</span>

<span class="sd">            - Lightning calls ``.backward()`` and ``.step()`` on each optimizer and learning rate scheduler as needed.</span>
<span class="sd">            - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizers.</span>
<span class="sd">            - If you use multiple optimizers, :meth:`training_step` will have an additional ``optimizer_idx`` parameter.</span>
<span class="sd">            - If you use :class:`torch.optim.LBFGS`, Lightning handles the closure function automatically for you.</span>
<span class="sd">            - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer</span>
<span class="sd">              at each training step.</span>
<span class="sd">            - If you need to control how often those optimizers step or override the default ``.step()`` schedule,</span>
<span class="sd">              override the :meth:`optimizer_step` hook.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">rank_zero_warn</span><span class="p">(</span><span class="s2">&quot;`configure_optimizers` must be implemented to be used with the Lightning Trainer&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">manual_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Call this directly from your :meth:`training_step` when doing optimizations manually. By using this,</span>
<span class="sd">        Lightning can ensure that all the proper scaling gets applied when using mixed precision.</span>

<span class="sd">        See :ref:`manual optimization&lt;common/optimization:Manual optimization&gt;` for more examples.</span>

<span class="sd">        Example::</span>

<span class="sd">            def training_step(...):</span>
<span class="sd">                opt = self.optimizers()</span>
<span class="sd">                loss = ...</span>
<span class="sd">                opt.zero_grad()</span>
<span class="sd">                # automatically applies scaling, etc...</span>
<span class="sd">                self.manual_backward(loss)</span>
<span class="sd">                opt.step()</span>

<span class="sd">        Args:</span>
<span class="sd">            loss: The tensor on which to compute gradients. Must have a graph attached.</span>
<span class="sd">            *args: Additional positional arguments to be forwarded to :meth:`~torch.Tensor.backward`</span>
<span class="sd">            **kwargs: Additional keyword arguments to be forwarded to :meth:`~torch.Tensor.backward`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_verify_is_manual_optimization</span><span class="p">(</span><span class="s2">&quot;manual_backward&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">],</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your</span>
<span class="sd">        own implementation if you need to.</span>

<span class="sd">        Args:</span>
<span class="sd">            loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here</span>
<span class="sd">                holds the normalized value (scaled by 1 / accumulation steps).</span>
<span class="sd">            optimizer: Current optimizer being used. ``None`` if using manual optimization.</span>
<span class="sd">            optimizer_idx: Index of the current optimizer being used. ``None`` if using manual optimization.</span>

<span class="sd">        Example::</span>

<span class="sd">            def backward(self, loss, optimizer, optimizer_idx):</span>
<span class="sd">                loss.backward()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">toggle_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">],</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Makes sure only the gradients of the current optimizer&#39;s parameters are calculated in the training step</span>
<span class="sd">        to prevent dangling gradients in multiple-optimizer setup.</span>

<span class="sd">        This is only called automatically when automatic optimization is enabled and multiple optimizers are used.</span>
<span class="sd">        It works with :meth:`untoggle_optimizer` to make sure ``param_requires_grad_state`` is properly reset.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer: The optimizer to toggle.</span>
<span class="sd">            optimizer_idx: The index of the optimizer to toggle.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Iterate over all optimizer parameters to preserve their `requires_grad` information</span>
        <span class="c1"># in case these are pre-defined during `configure_optimizers`</span>
        <span class="n">param_requires_grad_state</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">opt</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">optimizers</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                    <span class="c1"># If a param already appear in param_requires_grad_state, continue</span>
                    <span class="k">if</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_requires_grad_state</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">param_requires_grad_state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Then iterate over the current optimizer&#39;s parameters and set its `requires_grad`</span>
        <span class="c1"># properties accordingly</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">param_requires_grad_state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span> <span class="o">=</span> <span class="n">param_requires_grad_state</span>

    <span class="k">def</span> <span class="nf">untoggle_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`.</span>

<span class="sd">        This is only called automatically when automatic optimization is enabled and multiple optimizers are used.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer_idx: The index of the optimizer to untoggle.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">opt_idx</span><span class="p">,</span> <span class="n">opt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">optimizers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">!=</span> <span class="n">opt_idx</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                        <span class="k">if</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span><span class="p">:</span>
                            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
        <span class="c1"># save memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">clip_gradients</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">gradient_clip_val</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">gradient_clip_algorithm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Handles gradient clipping internally.</span>

<span class="sd">        Note:</span>
<span class="sd">            Do not override this method. If you want to customize gradient clipping, consider</span>
<span class="sd">            using :meth:`configure_gradient_clipping` method.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer: Current optimizer being used.</span>
<span class="sd">            gradient_clip_val: The value at which to clip gradients.</span>
<span class="sd">            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=&quot;value&quot;``</span>
<span class="sd">                to clip by value, and ``gradient_clip_algorithm=&quot;norm&quot;`` to clip by norm.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">gradient_clip_val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">gradient_clip_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_val</span> <span class="ow">or</span> <span class="mf">0.0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_val</span> <span class="o">!=</span> <span class="n">gradient_clip_val</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You have set `Trainer(gradient_clip_val=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_val</span><span class="si">!r}</span><span class="s2">)`&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; and have passed `clip_gradients(gradient_clip_val=</span><span class="si">{</span><span class="n">gradient_clip_val</span><span class="si">!r}</span><span class="s2">)`.&quot;</span>
                <span class="s2">&quot; Please use only one of them.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">gradient_clip_algorithm</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">gradient_clip_algorithm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_algorithm</span> <span class="ow">or</span> <span class="s2">&quot;norm&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gradient_clip_algorithm</span> <span class="o">=</span> <span class="n">gradient_clip_algorithm</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_algorithm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_algorithm</span> <span class="o">!=</span> <span class="n">gradient_clip_algorithm</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;You have set `Trainer(gradient_clip_algorithm=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_algorithm</span><span class="o">.</span><span class="n">value</span><span class="si">!r}</span><span class="s2">)`&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; and have passed `clip_gradients(gradient_clip_algorithm=</span><span class="si">{</span><span class="n">gradient_clip_algorithm</span><span class="si">!r}</span><span class="s2">)&quot;</span>
                    <span class="s2">&quot; Please use only one of them.&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gradient_clip_val</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`gradient_clip_val` should be an int or a float. Got </span><span class="si">{</span><span class="n">gradient_clip_val</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">GradClipAlgorithmType</span><span class="o">.</span><span class="n">supported_type</span><span class="p">(</span><span class="n">gradient_clip_algorithm</span><span class="o">.</span><span class="n">lower</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`gradient_clip_algorithm` </span><span class="si">{</span><span class="n">gradient_clip_algorithm</span><span class="si">}</span><span class="s2"> is invalid.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; Allowed algorithms: </span><span class="si">{</span><span class="n">GradClipAlgorithmType</span><span class="o">.</span><span class="n">supported_types</span><span class="p">()</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="n">gradient_clip_algorithm</span> <span class="o">=</span> <span class="n">GradClipAlgorithmType</span><span class="p">(</span><span class="n">gradient_clip_algorithm</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">precision_plugin</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">configure_gradient_clipping</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">gradient_clip_val</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">gradient_clip_algorithm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Perform gradient clipping for the optimizer parameters. Called before :meth:`optimizer_step`.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer: Current optimizer being used.</span>
<span class="sd">            optimizer_idx: Index of the current optimizer being used.</span>
<span class="sd">            gradient_clip_val: The value at which to clip gradients. By default value passed in Trainer</span>
<span class="sd">                will be available here.</span>
<span class="sd">            gradient_clip_algorithm: The gradient clipping algorithm to use. By default value</span>
<span class="sd">                passed in Trainer will be available here.</span>

<span class="sd">        Example::</span>

<span class="sd">            # Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN</span>
<span class="sd">            def configure_gradient_clipping(self, optimizer, optimizer_idx, gradient_clip_val, gradient_clip_algorithm):</span>
<span class="sd">                if optimizer_idx == 1:</span>
<span class="sd">                    # Lightning will handle the gradient clipping</span>
<span class="sd">                    self.clip_gradients(</span>
<span class="sd">                        optimizer,</span>
<span class="sd">                        gradient_clip_val=gradient_clip_val,</span>
<span class="sd">                        gradient_clip_algorithm=gradient_clip_algorithm</span>
<span class="sd">                    )</span>
<span class="sd">                else:</span>
<span class="sd">                    # implement your own custom logic to clip gradients for generator (optimizer_idx=0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="n">gradient_clip_algorithm</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">lr_scheduler_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">scheduler</span><span class="p">:</span> <span class="n">LRSchedulerTypeUnion</span><span class="p">,</span>
        <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">metric</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Override this method to adjust the default way the</span>
<span class="sd">        :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each scheduler.</span>
<span class="sd">        By default, Lightning calls ``step()`` and as shown in the example</span>
<span class="sd">        for each scheduler based on its ``interval``.</span>

<span class="sd">        Args:</span>
<span class="sd">            scheduler: Learning rate scheduler.</span>
<span class="sd">            optimizer_idx: Index of the optimizer associated with this scheduler.</span>
<span class="sd">            metric: Value of the monitor used for schedulers like ``ReduceLROnPlateau``.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # DEFAULT</span>
<span class="sd">            def lr_scheduler_step(self, scheduler, optimizer_idx, metric):</span>
<span class="sd">                if metric is None:</span>
<span class="sd">                    scheduler.step()</span>
<span class="sd">                else:</span>
<span class="sd">                    scheduler.step(metric)</span>

<span class="sd">            # Alternative way to update schedulers if it requires an epoch value</span>
<span class="sd">            def lr_scheduler_step(self, scheduler, optimizer_idx, metric):</span>
<span class="sd">                scheduler.step(epoch=self.current_epoch)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">metric</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">metric</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">],</span>
        <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">optimizer_closure</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">on_tpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">using_native_amp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">using_lbfgs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls</span>
<span class="sd">        each optimizer.</span>

<span class="sd">        By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example once per optimizer.</span>
<span class="sd">        This method (and ``zero_grad()``) won&#39;t be called during the accumulation phase when</span>
<span class="sd">        ``Trainer(accumulate_grad_batches != 1)``. Overriding this hook has no benefit with manual optimization.</span>

<span class="sd">        Args:</span>
<span class="sd">            epoch: Current epoch</span>
<span class="sd">            batch_idx: Index of current batch</span>
<span class="sd">            optimizer: A PyTorch optimizer</span>
<span class="sd">            optimizer_idx: If you used multiple optimizers, this indexes into that list.</span>
<span class="sd">            optimizer_closure: The optimizer closure. This closure must be executed as it includes the</span>
<span class="sd">                calls to ``training_step()``, ``optimizer.zero_grad()``, and ``backward()``.</span>
<span class="sd">            on_tpu: ``True`` if TPU backward is required</span>
<span class="sd">            using_native_amp: ``True`` if using native amp</span>
<span class="sd">            using_lbfgs: True if the matching optimizer is :class:`torch.optim.LBFGS`</span>

<span class="sd">        Examples::</span>

<span class="sd">            # DEFAULT</span>
<span class="sd">            def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,</span>
<span class="sd">                               optimizer_closure, on_tpu, using_native_amp, using_lbfgs):</span>
<span class="sd">                optimizer.step(closure=optimizer_closure)</span>

<span class="sd">            # Alternating schedule for optimizer steps (i.e.: GANs)</span>
<span class="sd">            def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,</span>
<span class="sd">                               optimizer_closure, on_tpu, using_native_amp, using_lbfgs):</span>
<span class="sd">                # update generator opt every step</span>
<span class="sd">                if optimizer_idx == 0:</span>
<span class="sd">                    optimizer.step(closure=optimizer_closure)</span>

<span class="sd">                # update discriminator opt every 2 steps</span>
<span class="sd">                if optimizer_idx == 1:</span>
<span class="sd">                    if (batch_idx + 1) % 2 == 0 :</span>
<span class="sd">                        optimizer.step(closure=optimizer_closure)</span>
<span class="sd">                    else:</span>
<span class="sd">                        # call the closure by itself to run `training_step` + `backward` without an optimizer step</span>
<span class="sd">                        optimizer_closure()</span>

<span class="sd">                # ...</span>
<span class="sd">                # add as many optimizers as you want</span>

<span class="sd">        Here&#39;s another example showing how to use this for more advanced things such as</span>
<span class="sd">        learning rate warm-up:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # learning rate warm-up</span>
<span class="sd">            def optimizer_step(</span>
<span class="sd">                self,</span>
<span class="sd">                epoch,</span>
<span class="sd">                batch_idx,</span>
<span class="sd">                optimizer,</span>
<span class="sd">                optimizer_idx,</span>
<span class="sd">                optimizer_closure,</span>
<span class="sd">                on_tpu,</span>
<span class="sd">                using_native_amp,</span>
<span class="sd">                using_lbfgs,</span>
<span class="sd">            ):</span>
<span class="sd">                # update params</span>
<span class="sd">                optimizer.step(closure=optimizer_closure)</span>

<span class="sd">                # manually warm up lr without a scheduler</span>
<span class="sd">                if self.trainer.global_step &lt; 500:</span>
<span class="sd">                    lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0)</span>
<span class="sd">                    for pg in optimizer.param_groups:</span>
<span class="sd">                        pg[&quot;lr&quot;] = lr_scale * self.learning_rate</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Override this method to change the default behaviour of ``optimizer.zero_grad()``.</span>

<span class="sd">        Args:</span>
<span class="sd">            epoch: Current epoch</span>
<span class="sd">            batch_idx: Index of current batch</span>
<span class="sd">            optimizer: A PyTorch optimizer</span>
<span class="sd">            optimizer_idx: If you used multiple optimizers this indexes into that list.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # DEFAULT</span>
<span class="sd">            def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):</span>
<span class="sd">                optimizer.zero_grad()</span>

<span class="sd">            # Set gradients to `None` instead of zero to improve performance.</span>
<span class="sd">            def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):</span>
<span class="sd">                optimizer.zero_grad(set_to_none=True)</span>

<span class="sd">        See :meth:`torch.optim.Optimizer.zero_grad` for the explanation of the above example.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">tbptt_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">split_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        When using truncated backpropagation through time, each batch must be split along the</span>
<span class="sd">        time dimension. Lightning handles this by default, but for custom behavior override</span>
<span class="sd">        this function.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: Current batch</span>
<span class="sd">            split_size: The size of the split</span>

<span class="sd">        Return:</span>
<span class="sd">            List of batch splits. Each split will be passed to :meth:`training_step` to enable truncated</span>
<span class="sd">            back propagation through time. The default implementation splits root level Tensors and</span>
<span class="sd">            Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</span>

<span class="sd">        Examples::</span>

<span class="sd">            def tbptt_split_batch(self, batch, split_size):</span>
<span class="sd">                splits = []</span>
<span class="sd">                for t in range(0, time_dims[0], split_size):</span>
<span class="sd">                    batch_split = []</span>
<span class="sd">                    for i, x in enumerate(batch):</span>
<span class="sd">                        if isinstance(x, torch.Tensor):</span>
<span class="sd">                            split_x = x[:, t:t + split_size]</span>
<span class="sd">                        elif isinstance(x, collections.Sequence):</span>
<span class="sd">                            split_x = [None] * len(x)</span>
<span class="sd">                            for batch_idx in range(len(x)):</span>
<span class="sd">                              split_x[batch_idx] = x[batch_idx][t:t + split_size]</span>
<span class="sd">                        batch_split.append(split_x)</span>
<span class="sd">                    splits.append(batch_split)</span>
<span class="sd">                return splits</span>

<span class="sd">        Note:</span>
<span class="sd">            Called in the training loop after</span>
<span class="sd">            :meth:`~pytorch_lightning.callbacks.base.Callback.on_train_batch_start`</span>
<span class="sd">            if :paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps` &gt; 0.</span>
<span class="sd">            Each returned batch split is passed separately to :meth:`training_step`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">time_dims</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">))]</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">time_dims</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Unable to determine batch time dimension&quot;</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">time_dims</span><span class="p">),</span> <span class="s2">&quot;Batch time dimension length is ambiguous&quot;</span>

        <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">split_size</span><span class="p">):</span>
            <span class="n">batch_split</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">split_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t</span> <span class="p">:</span> <span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
                    <span class="n">split_x</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                        <span class="n">split_x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">][</span><span class="n">t</span> <span class="p">:</span> <span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>

                <span class="n">batch_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_x</span><span class="p">)</span>

            <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_split</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">splits</span>

    <span class="k">def</span> <span class="nf">summarize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelSummary</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Summarize this LightningModule.</span>

<span class="sd">        .. deprecated:: v1.5</span>
<span class="sd">            This method was deprecated in v1.5 in favor of `pytorch_lightning.utilities.model_summary.summarize`</span>
<span class="sd">            and will be removed in v1.7.</span>

<span class="sd">        Args:</span>
<span class="sd">            max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the</span>
<span class="sd">                layer summary off. Default: 1.</span>

<span class="sd">        Return:</span>
<span class="sd">            The model summary object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">rank_zero_deprecation</span><span class="p">(</span>
            <span class="s2">&quot;The `LightningModule.summarize` method is deprecated in v1.5 and will be removed in v1.7. &quot;</span>
            <span class="s2">&quot;Use `pytorch_lightning.utilities.model_summary.summarize` instead.&quot;</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">summarize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Freeze all params for inference.</span>

<span class="sd">        Example::</span>

<span class="sd">            model = MyLightningModule(...)</span>
<span class="sd">            model.freeze()</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">unfreeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Unfreeze all parameters for training.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            model = MyLightningModule(...)</span>
<span class="sd">            model.unfreeze()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_progress_bar_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        .. deprecated:: v1.5</span>
<span class="sd">            This method was deprecated in v1.5 in favor of</span>
<span class="sd">            `pytorch_lightning.callbacks.progress.base.get_metrics` and will be removed in v1.7.</span>

<span class="sd">        Implement this to override the default items displayed in the progress bar.</span>
<span class="sd">        By default it includes the average loss value, split index of BPTT (if used)</span>
<span class="sd">        and the version of the experiment when using a logger.</span>

<span class="sd">        .. code-block::</span>

<span class="sd">            Epoch 1:   4%|▎         | 40/1095 [00:03&lt;01:37, 10.84it/s, loss=4.501, v_num=10]</span>

<span class="sd">        Here is an example how to override the defaults:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            def get_progress_bar_dict(self):</span>
<span class="sd">                # don&#39;t show the version number</span>
<span class="sd">                items = super().get_progress_bar_dict()</span>
<span class="sd">                items.pop(&quot;v_num&quot;, None)</span>
<span class="sd">                return items</span>

<span class="sd">        Return:</span>
<span class="sd">            Dictionary with the items to be displayed in the progress bar.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">progress_base</span><span class="o">.</span><span class="n">get_standard_metrics</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_verify_is_manual_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;to use </span><span class="si">{</span><span class="n">fn_name</span><span class="si">}</span><span class="s2">, please disable automatic optimization:&quot;</span>
                <span class="s2">&quot; set model property `automatic_optimization` as False&quot;</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_auto_collect_arguments</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">frame</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Collect all module arguments in the current constructor and all child constructors. The child</span>
<span class="sd">        constructors are all the ``__init__`` methods that reach the current class through (chained)</span>
<span class="sd">        ``super().__init__()`` calls.</span>

<span class="sd">        Args:</span>
<span class="sd">            frame: instance frame</span>

<span class="sd">        Returns:</span>
<span class="sd">            self_arguments: arguments dictionary of the first instance</span>
<span class="sd">            parents_arguments: arguments dictionary of the parent&#39;s instances</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">frame</span><span class="p">:</span>
            <span class="n">frame</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">currentframe</span><span class="p">()</span>

        <span class="n">frame_args</span> <span class="o">=</span> <span class="n">collect_init_args</span><span class="p">(</span><span class="n">frame</span><span class="o">.</span><span class="n">f_back</span><span class="p">,</span> <span class="p">[])</span>
        <span class="n">self_arguments</span> <span class="o">=</span> <span class="n">frame_args</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># set hyper_parameters in child</span>
        <span class="n">self_arguments</span> <span class="o">=</span> <span class="n">self_arguments</span>
        <span class="n">parents_arguments</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># add all arguments from parents</span>
        <span class="k">for</span> <span class="n">args</span> <span class="ow">in</span> <span class="n">frame_args</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">parents_arguments</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self_arguments</span><span class="p">,</span> <span class="n">parents_arguments</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">to_onnx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">],</span> <span class="n">input_sample</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Saves the model in ONNX format.</span>

<span class="sd">        Args:</span>
<span class="sd">            file_path: The path of the file the onnx model should be saved to.</span>
<span class="sd">            input_sample: An input for tracing. Default: None (Use self.example_input_array)</span>
<span class="sd">            **kwargs: Will be passed to torch.onnx.export function.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; class SimpleModel(LightningModule):</span>
<span class="sd">            ...     def __init__(self):</span>
<span class="sd">            ...         super().__init__()</span>
<span class="sd">            ...         self.l1 = torch.nn.Linear(in_features=64, out_features=4)</span>
<span class="sd">            ...</span>
<span class="sd">            ...     def forward(self, x):</span>
<span class="sd">            ...         return torch.relu(self.l1(x.view(x.size(0), -1)))</span>

<span class="sd">            &gt;&gt;&gt; with tempfile.NamedTemporaryFile(suffix=&#39;.onnx&#39;, delete=False) as tmpfile:</span>
<span class="sd">            ...     model = SimpleModel()</span>
<span class="sd">            ...     input_sample = torch.randn((1, 64))</span>
<span class="sd">            ...     model.to_onnx(tmpfile.name, input_sample, export_params=True)</span>
<span class="sd">            ...     os.path.isfile(tmpfile.name)</span>
<span class="sd">            True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span>

        <span class="k">if</span> <span class="n">input_sample</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Could not export to ONNX since neither `input_sample` nor&quot;</span>
                    <span class="s2">&quot; `model.example_input_array` attribute is set.&quot;</span>
                <span class="p">)</span>
            <span class="n">input_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span>

        <span class="n">input_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_batch_transfer_handler</span><span class="p">(</span><span class="n">input_sample</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">_TORCH_GREATER_EQUAL_1_10</span> <span class="ow">and</span> <span class="s2">&quot;example_outputs&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_sample</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">):</span>
                <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;example_outputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">*</span><span class="n">input_sample</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;example_outputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">input_sample</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">file_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">to_torchscript</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">file_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;script&quot;</span><span class="p">,</span>
        <span class="n">example_inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">ScriptModule</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ScriptModule</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing,</span>
<span class="sd">        please provided the argument ``method=&#39;trace&#39;`` and make sure that either the `example_inputs` argument is</span>
<span class="sd">        provided, or the model has :attr:`example_input_array` set. If you would like to customize the modules that</span>
<span class="sd">        are scripted you should override this method. In case you want to return multiple modules, we recommend</span>
<span class="sd">        using a dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            file_path: Path where to save the torchscript. Default: None (no file saved).</span>
<span class="sd">            method: Whether to use TorchScript&#39;s script or trace method. Default: &#39;script&#39;</span>
<span class="sd">            example_inputs: An input to be used to do tracing when method is set to &#39;trace&#39;.</span>
<span class="sd">              Default: None (uses :attr:`example_input_array`)</span>
<span class="sd">            **kwargs: Additional arguments that will be passed to the :func:`torch.jit.script` or</span>
<span class="sd">              :func:`torch.jit.trace` function.</span>

<span class="sd">        Note:</span>
<span class="sd">            - Requires the implementation of the</span>
<span class="sd">              :meth:`~pytorch_lightning.core.lightning.LightningModule.forward` method.</span>
<span class="sd">            - The exported script will be set to evaluation mode.</span>
<span class="sd">            - It is recommended that you install the latest supported version of PyTorch</span>
<span class="sd">              to use this feature without limitations. See also the :mod:`torch.jit`</span>
<span class="sd">              documentation for supported features.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; class SimpleModel(LightningModule):</span>
<span class="sd">            ...     def __init__(self):</span>
<span class="sd">            ...         super().__init__()</span>
<span class="sd">            ...         self.l1 = torch.nn.Linear(in_features=64, out_features=4)</span>
<span class="sd">            ...</span>
<span class="sd">            ...     def forward(self, x):</span>
<span class="sd">            ...         return torch.relu(self.l1(x.view(x.size(0), -1)))</span>
<span class="sd">            ...</span>
<span class="sd">            &gt;&gt;&gt; model = SimpleModel()</span>
<span class="sd">            &gt;&gt;&gt; model.to_torchscript(file_path=&quot;model.pt&quot;)  # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; os.path.isfile(&quot;model.pt&quot;)  # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; torch.jit.save(model.to_torchscript(file_path=&quot;model_trace.pt&quot;, method=&#39;trace&#39;, # doctest: +SKIP</span>
<span class="sd">            ...                                     example_inputs=torch.randn(1, 64)))  # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; os.path.isfile(&quot;model_trace.pt&quot;)  # doctest: +SKIP</span>
<span class="sd">            True</span>

<span class="sd">        Return:</span>
<span class="sd">            This LightningModule as a torchscript, regardless of whether `file_path` is</span>
<span class="sd">            defined or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;script&quot;</span><span class="p">:</span>
            <span class="n">torchscript_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;trace&quot;</span><span class="p">:</span>
            <span class="c1"># if no example inputs are provided, try to see if model has example_input_array set</span>
            <span class="k">if</span> <span class="n">example_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Choosing method=`trace` requires either `example_inputs`&quot;</span>
                        <span class="s2">&quot; or `model.example_input_array` to be defined.&quot;</span>
                    <span class="p">)</span>
                <span class="n">example_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span>

            <span class="c1"># automatically send example inputs to the right device and use trace</span>
            <span class="n">example_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_batch_transfer_handler</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">)</span>
            <span class="n">torchscript_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">example_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The &#39;method&#39; parameter only supports &#39;script&#39; or &#39;trace&#39;, but value given was: </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">file_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fs</span> <span class="o">=</span> <span class="n">get_filesystem</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">fs</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">torchscript_module</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">return</span> <span class="n">torchscript_module</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">model_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the model size in MegaBytes (MB)</span>

<span class="sd">        Note:</span>
<span class="sd">            This property will not return correct value for Deepspeed (stage 3) and fully-sharded training.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span><span class="p">:</span>  <span class="c1"># remove with the deprecation removal</span>
            <span class="n">rank_zero_deprecation</span><span class="p">(</span>
                <span class="s2">&quot;The `LightningModule.model_size` property was deprecated in v1.5 and will be removed in v1.7.&quot;</span>
                <span class="s2">&quot; Please use the `pytorch_lightning.utilities.memory.get_model_size_mb`.&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">get_model_size_mb</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">use_amp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        .. deprecated:: v1.6.</span>

<span class="sd">            This property was deprecated in v1.6 and will be removed in v1.8.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span><span class="p">:</span>  <span class="c1"># remove with the deprecation removal</span>
            <span class="n">rank_zero_deprecation</span><span class="p">(</span>
                <span class="s2">&quot;`LightningModule.use_amp` was deprecated in v1.6 and will be removed in v1.8.&quot;</span>
                <span class="s2">&quot; Please use `Trainer.amp_backend`.&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_amp</span>

    <span class="nd">@use_amp</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">use_amp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_amp</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        .. deprecated:: v1.6.</span>

<span class="sd">            This property was deprecated in v1.6 and will be removed in v1.8.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span><span class="p">:</span>  <span class="c1"># remove with the deprecation removal</span>
            <span class="n">rank_zero_deprecation</span><span class="p">(</span>
                <span class="s2">&quot;`LightningModule.use_amp` was deprecated in v1.6 and will be removed in v1.8.&quot;</span>
                <span class="s2">&quot; Please use `Trainer.amp_backend`.&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_amp</span> <span class="o">=</span> <span class="n">use_amp</span>

    <span class="k">def</span> <span class="nf">add_to_queue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queue</span><span class="p">:</span> <span class="n">pl</span><span class="o">.</span><span class="n">strategies</span><span class="o">.</span><span class="n">launchers</span><span class="o">.</span><span class="n">spawn</span><span class="o">.</span><span class="n">_FakeQueue</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Appends the :attr:`trainer.callback_metrics` dictionary to the given queue. To avoid issues with memory</span>
<span class="sd">        sharing, we cast the data to numpy.</span>

<span class="sd">        Args:</span>
<span class="sd">            queue: the instance of the queue to append the data.</span>

<span class="sd">        .. deprecated:: v1.5</span>
<span class="sd">            This method was deprecated in v1.5 and will be removed in v1.7.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">get_from_queue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queue</span><span class="p">:</span> <span class="n">pl</span><span class="o">.</span><span class="n">strategies</span><span class="o">.</span><span class="n">launchers</span><span class="o">.</span><span class="n">spawn</span><span class="o">.</span><span class="n">_FakeQueue</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue. To preserve consistency,</span>
<span class="sd">        we cast back the data to ``torch.Tensor``.</span>

<span class="sd">        Args:</span>
<span class="sd">            queue: the instance of the queue from where to get the data.</span>

<span class="sd">        .. deprecated:: v1.5</span>
<span class="sd">            This method was deprecated in v1.5 and will be removed in v1.7.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">_prevent_trainer_and_dataloaders_deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_should_prevent_trainer_and_dataloaders_deepcopy</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">yield</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_should_prevent_trainer_and_dataloaders_deepcopy</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_prevent_trainer_and_dataloaders_deepcopy</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;trainer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;train_dataloader&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;val_dataloader&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;test_dataloader&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;predict_dataloader&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">_register_sharded_tensor_state_dict_hooks_if_available</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Adds ShardedTensor state dict hooks if ShardedTensors are supported.</span>

<span class="sd">        These hooks ensure that ShardedTensors are included when saving, and are loaded the LightningModule correctly.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_TORCH_GREATER_EQUAL_1_10</span> <span class="ow">or</span> <span class="n">_IS_WINDOWS</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">rank_zero_debug</span><span class="p">(</span><span class="s2">&quot;Could not register sharded tensor state dict hooks&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="kn">from</span> <span class="nn">torch.distributed._sharded_tensor</span> <span class="kn">import</span> <span class="n">pre_load_state_dict_hook</span><span class="p">,</span> <span class="n">state_dict_hook</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_register_state_dict_hook</span><span class="p">(</span><span class="n">state_dict_hook</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">_TORCH_GREATER_EQUAL_1_12</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_register_load_state_dict_pre_hook</span><span class="p">(</span><span class="n">pre_load_state_dict_hook</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># We need to make sure the self inside the method is a weakref proxy</span>
            <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">_register_load_state_dict_pre_hook</span><span class="p">(</span><span class="n">weakref</span><span class="o">.</span><span class="n">proxy</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">pre_load_state_dict_hook</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2021, Chris Santiago
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/scripts/furo.js"></script>
    </body>
</html>