{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf331de-1030-4ec0-be04-57a61c3f3cdb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import transformers\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from emonet import RATINGS, DATA_DIR\n",
    "from emonet.data_loader import RandomSegment, SBAugment, get_datasets\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "202b1bc3-8599-4f42-9db4-659a1d637856",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "DATA_DIR = pathlib.Path('/datasets/emonet-data')\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7ab159f9-703b-4815-9416-86ddc3e2e8e2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tsfmr = nn.Sequential(\n",
    "        RandomSegment(seconds=29, sample_rate=16000),\n",
    "    )\n",
    "\n",
    "data = get_datasets('Michelle Lyn', DATA_DIR, transform=tsfmr)\n",
    "train_dl = DataLoader(data['train'], 4)\n",
    "valid_dl = DataLoader(data['valid'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cc15806e-4266-46c1-bc19-764603c814e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EmotionClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # todo make feature model an argument\n",
    "        self.wav2vec = transformers.Wav2Vec2Model.from_pretrained('audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim')\n",
    "        self.n_ratings = len(RATINGS)\n",
    "        self.nodes_fc1 = 128  # todo make this and any other nodes args\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_pipe = nn.Sequential(\n",
    "            RandomSegment(seconds=5), # todo parameterize\n",
    "            # SBAugment(perturb_prob=0.2, drop_freq_prob=0.2, drop_chunk_prob=0.2, speeds=[90, 110])\n",
    "        )\n",
    "\n",
    "        self.valid_pipe = nn.Sequential(\n",
    "            RandomSegment(seconds=5),\n",
    "        )\n",
    "\n",
    "        # todo experiment with dropout, etc.\n",
    "        self.anger = self.anger = nn.Sequential(  # todo maybe add apdativeAvgPooling to get consistent size\n",
    "            nn.LazyLinear(out_features=self.nodes_fc1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.nodes_fc1, out_features=self.n_ratings)\n",
    "        )\n",
    "\n",
    "        self.fear = nn.Sequential(\n",
    "            nn.LazyLinear(out_features=self.nodes_fc1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.nodes_fc1, out_features=self.n_ratings)\n",
    "        )\n",
    "\n",
    "        self.sadness = nn.Sequential(\n",
    "            nn.LazyLinear(out_features=self.nodes_fc1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.nodes_fc1, out_features=self.n_ratings)\n",
    "        )\n",
    "\n",
    "    def multi_loss(self, pred, actual):\n",
    "        loss = torch.zeros(1)\n",
    "        for key in actual:\n",
    "            loss += self.loss_func(pred[key], actual[key])  # todo please fix me\n",
    "        return loss\n",
    "\n",
    "    def pred_probas(self, outputs):\n",
    "        return {k: F.softmax(v, dim=1) for k, v in outputs.items()}\n",
    "\n",
    "    def pred_labels(self, outputs):\n",
    "        return {k: v.argmax(1) for k, v in self.pred_probas(outputs).items()}\n",
    "\n",
    "    def training_step(self, batch, idx):\n",
    "        x, labels = batch\n",
    "        bs = x.shape[0]\n",
    "        x = self.train_pipe(x)\n",
    "        feat = self.wav2vec(x)\n",
    "        x = feat.extract_features  # todo note the feature output changes based on input length\n",
    "        x.reshape(bs, -1)\n",
    "        outputs = {\n",
    "            'anger': self.anger(x),\n",
    "            'fear': self.fear(x),\n",
    "            'sadness': self.sadness(x)\n",
    "        }\n",
    "        loss = self.multi_loss(outputs, labels)\n",
    "        preds = self.pred_labels(outputs)\n",
    "        acc = accuracy_score(list(labels.values()), list(preds.values()))\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log_dict({'train_loss': loss, 'train_acc': acc}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, idx):\n",
    "        x, labels = batch\n",
    "        bs = x.shape[0]\n",
    "        x = self.valid_pipe(x)  # todo right now this is only scoring a single, 5-second sample. change to score multiple and averge preds\n",
    "        feat = self.wav2vec(x)\n",
    "        x = feat.extract_features  # todo note the feature output changes based on input length\n",
    "        x.reshape(bs, -1)\n",
    "        outputs = {\n",
    "            'anger': self.anger(x),\n",
    "            'fear': self.fear(x),\n",
    "            'sadness': self.sadness(x)\n",
    "        }\n",
    "        loss = self.multi_loss(outputs, labels)\n",
    "        preds = self.pred_labels(outputs)\n",
    "        acc = accuracy_score(list(labels.values()), list(preds.values()))\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log_dict({'val_loss': loss, 'val_acc': acc}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, idx):\n",
    "        x, labels = batch\n",
    "        bs = x.shape[0]\n",
    "        x = self.valid_pipe(x)\n",
    "        feat = self.wav2vec(x)\n",
    "        x = feat.extract_features  # todo note the feature output changes based on input length\n",
    "        x.reshape(bs, -1)\n",
    "        outputs = {\n",
    "            'anger': self.anger(x),\n",
    "            'fear': self.fear(x),\n",
    "            'sadness': self.sadness(x)\n",
    "        }\n",
    "        loss = self.multi_loss(outputs, labels)\n",
    "        preds = self.pred_labels(outputs)\n",
    "        acc = accuracy_score(list(labels.values()), list(preds.values()))\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log_dict({'test_loss': loss, 'test_acc': acc}, on_step=True, on_epoch=True,\n",
    "                      prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a8757c30-00b0-4bb5-afcb-967349e1edd5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim were not used when initializing Wav2Vec2Model: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = EmotionClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b153f0f1-b02d-4c01-b1cc-4200f0e37811",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't access the shape of an uninitialized parameter or buffer. This error usually happens in `load_state_dict` when trying to load an uninitialized parameter into an initialized one. Call `forward` to initialize the parameters before accessing their attributes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [132]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/emonet/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1044\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# CHOOSE OPTIMIZER\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# allow for lr schedulers as well\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_schedulers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_frequencies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_optimizers(model)\n\u001b[0;32m-> 1044\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pretrain_routine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;66;03m# callbacks\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_fit_end()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/emonet/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1149\u001b[0m, in \u001b[0;36mTrainer.run_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_global_zero \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_summary \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_summary \u001b[38;5;129;01min\u001b[39;00m ModelSummary\u001b[38;5;241m.\u001b[39mMODES:\n\u001b[0;32m-> 1149\u001b[0m         \u001b[43mref_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights_summary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m   1152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_summary can be None, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ModelSummary\u001b[38;5;241m.\u001b[39mMODES)\n\u001b[1;32m   1153\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/emonet/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py:1495\u001b[0m, in \u001b[0;36mLightningModule.summarize\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m ModelSummary\u001b[38;5;241m.\u001b[39mMODE_DEFAULT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelSummary:\n\u001b[1;32m   1494\u001b[0m     model_summary \u001b[38;5;241m=\u001b[39m ModelSummary(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[0;32m-> 1495\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_summary\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_summary\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/emonet/lib/python3.9/site-packages/pytorch_lightning/core/memory.py:238\u001b[0m, in \u001b[0;36mModelSummary.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    Makes a summary listing with:\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    Layer Name, Layer Type, Number of Parameters, Input Sizes, Output Sizes\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    235\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_summary))))],\n\u001b[1;32m    236\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_names],\n\u001b[1;32m    237\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_types],\n\u001b[0;32m--> 238\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(get_human_readable_count, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_nums\u001b[49m))],\n\u001b[1;32m    239\u001b[0m     ]\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mexample_input_array \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m         arrays\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_sizes])\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/emonet/lib/python3.9/site-packages/pytorch_lightning/core/memory.py:194\u001b[0m, in \u001b[0;36mModelSummary.param_nums\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparam_nums\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [layer\u001b[38;5;241m.\u001b[39mnum_parameters \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_summary\u001b[38;5;241m.\u001b[39mvalues()]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/emonet/lib/python3.9/site-packages/pytorch_lightning/core/memory.py:194\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparam_nums\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_parameters\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_summary\u001b[38;5;241m.\u001b[39mvalues()]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/emonet/lib/python3.9/site-packages/pytorch_lightning/core/memory.py:105\u001b[0m, in \u001b[0;36mLayerSummary.num_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124;03m\"\"\" Returns the number of parameters in this module. \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/emonet/lib/python3.9/site-packages/pytorch_lightning/core/memory.py:105\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124;03m\"\"\" Returns the number of parameters in this module. \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39mprod(\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/emonet/lib/python3.9/site-packages/torch/nn/parameter.py:91\u001b[0m, in \u001b[0;36mUninitializedTensorMixin.shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt access the shape of an uninitialized parameter or buffer. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis error usually happens in `load_state_dict` when trying to load \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124man uninitialized parameter into an initialized one. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCall `forward` to initialize the parameters before accessing their attributes.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't access the shape of an uninitialized parameter or buffer. This error usually happens in `load_state_dict` when trying to load an uninitialized parameter into an initialized one. Call `forward` to initialize the parameters before accessing their attributes."
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1)\n",
    "trainer.fit(model, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b1f14-afde-494c-aa65-8edd9ea6d8d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "54710e9f-c515-485d-ba91-7b139bbb98c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from speechbrain.lobes.augment import TimeDomainSpecAugment\n",
    "\n",
    "import random\n",
    "from emonet import EMOTIONS, RATINGS, SAMPLE_RATE\n",
    "\n",
    "\n",
    "def get_random_segment(wav: torch.Tensor, seconds=7, sample_rate=SAMPLE_RATE):\n",
    "    buffer = seconds * sample_rate\n",
    "    end = wav.shape[-1] - buffer  # should pull timesteps if dims=1 or dims=2, provided following (batch, timestep, channel) format\n",
    "    start = random.randint(0, end)\n",
    "    if wav.ndim > 1:\n",
    "        return wav[:, start:start+buffer]  # assumes (timestamp, channel)\n",
    "    return wav[start:start+buffer]\n",
    "\n",
    "\n",
    "class RandomSegment(nn.Module):\n",
    "    def __init__(self, seconds: int, sample_rate: int = SAMPLE_RATE):\n",
    "        self.seconds = seconds\n",
    "        self.sample_rate = sample_rate\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return get_random_segment(x, seconds=self.seconds, sample_rate=self.sample_rate)\n",
    "\n",
    "\n",
    "class SBAugment(TimeDomainSpecAugment):\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, :, None]  # speechbrain expects tensor shape (batch, timesteps, channels)\n",
    "        lengths = torch.ones(x.shape[0])\n",
    "        x = self.speed_perturb(x)\n",
    "        x = self.drop_freq(x)\n",
    "        x = self.drop_chunk(x, lengths)\n",
    "        return x.squeeze(-1)  # drop last dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b91b4a8e-5752-434a-b61d-f756653e7d09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80000])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb = SBAugment(perturb_prob=0.2, drop_freq_prob=0.2, drop_chunk_prob=0.2, speeds=[90, 110])\n",
    "sb(trans).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4bbacbc-15fe-4396-92fa-0d1ba6f0b641",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "it = iter(valid_dl)\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "306ec5f7-da5b-4f94-990d-55399d9ab62f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c73cebd-f1fd-4378-81ae-8210612ecbdb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0897, -0.0768, -0.0551,  ...,  0.0060,  0.0073,  0.0025],\n",
       "        [-0.0139, -0.0171, -0.0268,  ...,  0.0067,  0.0010, -0.0114],\n",
       "        [ 0.0179,  0.0188,  0.0190,  ...,  0.1192,  0.1345,  0.1336],\n",
       "        [-0.0044,  0.0084,  0.0142,  ...,  0.0002,  0.0008, -0.0170]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4672c40-65f9-4d5c-a515-3c110af5e914",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 464000])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f186f745-e7de-44fd-a1df-4f83180a100a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80000])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b8563d55-79ee-4b89-a464-f39337cf43e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_pipe = nn.Sequential(\n",
    "            RandomSegment(seconds=5), # todo parameterize\n",
    "            SBAugment(perturb_prob=0.2, drop_freq_prob=0.2, drop_chunk_prob=0.2, speeds=[90, 110])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5f61cd5b-fba7-4352-ac39-3151396010ad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trans = train_pipe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ac805198-7817-470c-96f7-77e50a6d1934",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 88000])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a0e5ce75-1e01-4b39-b7e8-f06612c32943",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim were not used when initializing Wav2Vec2Model: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "wav2vec = transformers.Wav2Vec2Model.from_pretrained('audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9c4635db-3afa-409a-8220-a619bdde1adb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feat = wav2vec(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1b4eec7d-03af-42df-a50d-ea7e1c15f96d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xf = feat.extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "651ef623-a947-4226-831d-ac424165421e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 274, 512])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cf4822ee-e683-4c4b-9df1-d0c5370681ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xf = xf.reshape(4, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4524746c-c8da-4a98-9f6a-1b695745e1af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 140288])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6929fccd-47c2-4cc7-8001-17697e14fbd4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophersantiago/opt/miniconda3/envs/emonet/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "anger = nn.Sequential(  # todo maybe add apdativeAvgPooling to get consistent size\n",
    "            nn.LazyLinear(out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=4)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "51937320-670b-44d3-a48a-0b0264a61163",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outputs = {\n",
    "            'anger': anger(xf),\n",
    "            'fear': anger(xf),\n",
    "            'sadness': anger(xf),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "80e1b4df-62eb-4986-9438-6954801f2f54",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': tensor([[ 0.2479, -0.1088,  0.1016, -0.1237],\n",
       "         [-0.1646,  0.8251,  0.4551, -0.1429],\n",
       "         [ 0.0192,  0.2802,  0.1375,  0.0393],\n",
       "         [ 0.1703,  0.4405,  0.6012,  0.0890]], grad_fn=<AddmmBackward0>),\n",
       " 'fear': tensor([[ 0.2479, -0.1088,  0.1016, -0.1237],\n",
       "         [-0.1646,  0.8251,  0.4551, -0.1429],\n",
       "         [ 0.0192,  0.2802,  0.1375,  0.0393],\n",
       "         [ 0.1703,  0.4405,  0.6012,  0.0890]], grad_fn=<AddmmBackward0>),\n",
       " 'sadness': tensor([[ 0.2479, -0.1088,  0.1016, -0.1237],\n",
       "         [-0.1646,  0.8251,  0.4551, -0.1429],\n",
       "         [ 0.0192,  0.2802,  0.1375,  0.0393],\n",
       "         [ 0.1703,  0.4405,  0.6012,  0.0890]], grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3dc89b30-dc2e-4cb9-9cc4-523a9ef7b937",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "def multi_loss(pred, actual):\n",
    "        loss = torch.zeros(1)\n",
    "        for key in actual:\n",
    "            loss += ce(pred[key], actual[key])  # todo please fix me\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9a044ef3-6952-4496-be83-ad9290b946f6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': tensor([0, 0, 0, 0]),\n",
       " 'fear': tensor([0, 1, 2, 0]),\n",
       " 'sadness': tensor([1, 0, 1, 1])}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "426a87c6-13fd-4f03-a8fc-870e3e62a895",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2667], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_loss(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f026b34-d8c8-44e9-9196-f78de0f46dc6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
